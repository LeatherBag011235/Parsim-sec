{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from polars import functions as pf\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME_LIST = ['Apple%2520Inc.%2520(AAPL)%2520(CIK%25200000320193)', 'MICROSOFT%2520CORP%2520(MSFT)%2520(CIK%25200000789019)', \n",
    "                     'BigCommerce%2520Holdings%252C%2520Inc.%2520(BIGC)%2520(CIK%25200001626450)', 'ROKU%252C%2520INC%2520(ROKU)%2520(CIK%25200001428439)', \n",
    "                     'JPMORGAN%2520CHASE%2520%2526%2520CO%2520(JPM%252C%2520AMJ%252C%2520AMJB%252C%2520JPM)%2520(CIK%25200000019617)', 'VISA%2520INC.%2520(V)%2520(CIK%25200001403161)', \n",
    "                     'Block%252C%2520Inc.%2520(SQ%252C%2520BSQKZ)%2520(CIK%25200001512673)', 'Robinhood%2520Markets%252C%2520Inc.%2520(HOOD)%2520(CIK%25200001783879)', \n",
    "                     'JOHNSON%252C%252C%2520JOHNSON%2520(JNJ)%2520(CIK%25200000200406)', 'PFIZER%2520INC%2520(PFE)%2520(CIK%25200000078003)', \n",
    "                     'Moderna%252C%2520Inc.%2520(MRNA)%2520(CIK%25200001682852)', 'Teladoc%2520Health%252C%2520Inc.%2520(TDOC)%2520(CIK%25200001477449)', \n",
    "                     'EXXON%2520MOBIL%2520CORP%2520(XOM)%2520(CIK%25200000034088)', 'CHEVRON%2520CORP%2520(CVX)%2520(CIK%25200000093410)', \n",
    "                     'FIRST%2520SOLAR%252C%2520INC.%2520(FSLR)%2520(CIK%25200001274494)', 'PLUG%2520POWER%2520INC%2520(PLUG)%2520(CIK%25200001093691)', \n",
    "                     'GENERAL%252CELECTRIC%2520CO%2520(GE)%2520(CIK%25200000040545)', '3M%2520CO%2520(MMM)%2520(CIK%25200000066740)', \n",
    "                     'CATERPILLAR%2520INC%2520(CAT)%2520(CIK%25200000018230)', 'FASTENAL%2520CO%2520(FAST)%2520(CIK%25200000815556)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (86_531, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Word</th><th>Seq_num</th><th>Word Count</th><th>Word Proportion</th><th>Average Proportion</th><th>Std Dev</th><th>Doc Count</th><th>Negative</th><th>Positive</th><th>Uncertainty</th><th>Litigious</th><th>Strong_Modal</th><th>Weak_Modal</th><th>Constraining</th><th>Syllables</th><th>Source</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;AARDVARK&quot;</td><td>1</td><td>354</td><td>1.5501e-8</td><td>1.4226e-8</td><td>0.000004</td><td>99</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;AARDVARKS&quot;</td><td>2</td><td>3</td><td>1.3136e-10</td><td>8.6538e-12</td><td>9.2417e-9</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;ABACI&quot;</td><td>3</td><td>9</td><td>3.9409e-10</td><td>1.1697e-10</td><td>5.2905e-8</td><td>7</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;ABACK&quot;</td><td>4</td><td>29</td><td>1.2698e-9</td><td>6.6547e-10</td><td>1.5951e-7</td><td>28</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;ABACUS&quot;</td><td>5</td><td>8570</td><td>3.7526e-7</td><td>3.8095e-7</td><td>0.000035</td><td>1108</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;ZYGOTE&quot;</td><td>86529</td><td>50</td><td>2.1894e-9</td><td>8.7293e-10</td><td>1.8860e-7</td><td>35</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;ZYGOTES&quot;</td><td>86530</td><td>1</td><td>4.3788e-11</td><td>1.8095e-11</td><td>1.9324e-8</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>2</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;ZYGOTIC&quot;</td><td>86531</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;ZYMURGIES&quot;</td><td>86532</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>&quot;12of12inf&quot;</td></tr><tr><td>&quot;ZYMURGY&quot;</td><td>86533</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>3</td><td>&quot;12of12inf&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (86_531, 16)\n",
       "┌───────────┬─────────┬───────┬─────────────┬───┬─────────────┬────────────┬───────────┬───────────┐\n",
       "│ Word      ┆ Seq_num ┆ Word  ┆ Word        ┆ … ┆ Weak_Modal  ┆ Constraini ┆ Syllables ┆ Source    │\n",
       "│ ---       ┆ ---     ┆ Count ┆ Proportion  ┆   ┆ ---         ┆ ng         ┆ ---       ┆ ---       │\n",
       "│ str       ┆ i64     ┆ ---   ┆ ---         ┆   ┆ i64         ┆ ---        ┆ i64       ┆ str       │\n",
       "│           ┆         ┆ i64   ┆ f64         ┆   ┆             ┆ i64        ┆           ┆           │\n",
       "╞═══════════╪═════════╪═══════╪═════════════╪═══╪═════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ AARDVARK  ┆ 1       ┆ 354   ┆ 1.5501e-8   ┆ … ┆ 0           ┆ 0          ┆ 2         ┆ 12of12inf │\n",
       "│ AARDVARKS ┆ 2       ┆ 3     ┆ 1.3136e-10  ┆ … ┆ 0           ┆ 0          ┆ 2         ┆ 12of12inf │\n",
       "│ ABACI     ┆ 3       ┆ 9     ┆ 3.9409e-10  ┆ … ┆ 0           ┆ 0          ┆ 3         ┆ 12of12inf │\n",
       "│ ABACK     ┆ 4       ┆ 29    ┆ 1.2698e-9   ┆ … ┆ 0           ┆ 0          ┆ 2         ┆ 12of12inf │\n",
       "│ ABACUS    ┆ 5       ┆ 8570  ┆ 3.7526e-7   ┆ … ┆ 0           ┆ 0          ┆ 3         ┆ 12of12inf │\n",
       "│ …         ┆ …       ┆ …     ┆ …           ┆ … ┆ …           ┆ …          ┆ …         ┆ …         │\n",
       "│ ZYGOTE    ┆ 86529   ┆ 50    ┆ 2.1894e-9   ┆ … ┆ 0           ┆ 0          ┆ 2         ┆ 12of12inf │\n",
       "│ ZYGOTES   ┆ 86530   ┆ 1     ┆ 4.3788e-11  ┆ … ┆ 0           ┆ 0          ┆ 2         ┆ 12of12inf │\n",
       "│ ZYGOTIC   ┆ 86531   ┆ 0     ┆ 0.0         ┆ … ┆ 0           ┆ 0          ┆ 3         ┆ 12of12inf │\n",
       "│ ZYMURGIES ┆ 86532   ┆ 0     ┆ 0.0         ┆ … ┆ 0           ┆ 0          ┆ 3         ┆ 12of12inf │\n",
       "│ ZYMURGY   ┆ 86533   ┆ 0     ┆ 0.0         ┆ … ┆ 0           ┆ 0          ┆ 3         ┆ 12of12inf │\n",
       "└───────────┴─────────┴───────┴─────────────┴───┴─────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dict = pl.read_csv(r'C:\\Users\\310\\Desktop\\Progects_Py\\Parsim-sec\\src\\Analysis\\Loughran-McDonald_MasterDictionary_1993-2021.csv')\n",
    "lm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\Apple%2520Inc.%2520(AAPL)%2520(CIK%25200000320193)_reports.parquet\n",
      "MICROSOFT\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\MICROSOFT%2520CORP%2520(MSFT)%2520(CIK%25200000789019)_reports.parquet\n",
      "BigCommerce\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\BigCommerce%2520Holdings%252C%2520Inc.%2520(BIGC)%2520(CIK%25200001626450)_reports.parquet\n",
      "ROKU\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\ROKU%252C%2520INC%2520(ROKU)%2520(CIK%25200001428439)_reports.parquet\n",
      "JPMORGAN\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\JPMORGAN%2520CHASE%2520%2526%2520CO%2520(JPM%252C%2520AMJ%252C%2520AMJB%252C%2520JPM)%2520(CIK%25200000019617)_reports.parquet\n",
      "VISA\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\VISA%2520INC.%2520(V)%2520(CIK%25200001403161)_reports.parquet\n",
      "Block\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\Block%252C%2520Inc.%2520(SQ%252C%2520BSQKZ)%2520(CIK%25200001512673)_reports.parquet\n",
      "Robinhood\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\Robinhood%2520Markets%252C%2520Inc.%2520(HOOD)%2520(CIK%25200001783879)_reports.parquet\n",
      "JOHNSON\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\JOHNSON%252C%252C%2520JOHNSON%2520(JNJ)%2520(CIK%25200000200406)_reports.parquet\n",
      "PFIZER\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\PFIZER%2520INC%2520(PFE)%2520(CIK%25200000078003)_reports.parquet\n",
      "Moderna\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\Moderna%252C%2520Inc.%2520(MRNA)%2520(CIK%25200001682852)_reports.parquet\n",
      "Teladoc\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\Teladoc%2520Health%252C%2520Inc.%2520(TDOC)%2520(CIK%25200001477449)_reports.parquet\n",
      "EXXON\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\EXXON%2520MOBIL%2520CORP%2520(XOM)%2520(CIK%25200000034088)_reports.parquet\n",
      "CHEVRON\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\CHEVRON%2520CORP%2520(CVX)%2520(CIK%25200000093410)_reports.parquet\n",
      "FIRST\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\FIRST%2520SOLAR%252C%2520INC.%2520(FSLR)%2520(CIK%25200001274494)_reports.parquet\n",
      "PLUG\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\PLUG%2520POWER%2520INC%2520(PLUG)%2520(CIK%25200001093691)_reports.parquet\n",
      "GENERAL\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\GENERAL%252CELECTRIC%2520CO%2520(GE)%2520(CIK%25200000040545)_reports.parquet\n",
      "3M\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\3M%2520CO%2520(MMM)%2520(CIK%25200000066740)_reports.parquet\n",
      "CATERPILLAR\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\CATERPILLAR%2520INC%2520(CAT)%2520(CIK%25200000018230)_reports.parquet\n",
      "FASTENAL\n",
      "C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\\FASTENAL%2520CO%2520(FAST)%2520(CIK%25200000815556)_reports.parquet\n"
     ]
    }
   ],
   "source": [
    "#old version for dfs without 10-K reports, full text (downloaded, from div)\n",
    "df_dict = {}\n",
    "\n",
    "for company_name in COMPANY_NAME_LIST:\n",
    "\n",
    "    clean_name = company_name.split('%')[0]\n",
    "    print(clean_name)\n",
    "\n",
    "    df_name = f\"df_{clean_name}\"\n",
    "\n",
    "    file_path_partial = os.path.join(r\"C:\\Users\\310\\Desktop\\Progects_Py\\prepared_data\\full_10Q_10K_without_tabels\", company_name)\n",
    "    file_path = file_path_partial + '_reports.parquet'\n",
    "    print(file_path)\n",
    "\n",
    "    df = pl.read_parquet(file_path)\n",
    "\n",
    "    df_dict[df_name] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n"
     ]
    }
   ],
   "source": [
    "total_num_of_rep = 0\n",
    "\n",
    "for value in df_dict.values():\n",
    "    total_num_of_rep += value.width\n",
    "print(total_num_of_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13103235\n"
     ]
    }
   ],
   "source": [
    "total_num_of_words = 0\n",
    "\n",
    "for value in df_dict.values():\n",
    "    for col in value:\n",
    "        total_num_of_words += col.count()\n",
    "\n",
    "print(total_num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average doc length in words: 33342\n"
     ]
    }
   ],
   "source": [
    "#average doc length\n",
    "average_word_count = total_num_of_words/total_num_of_rep\n",
    "print(f'Average doc length in words: {round(average_word_count)}')\n",
    "ln_average_word_count = math.log(average_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2692\n"
     ]
    }
   ],
   "source": [
    "#ENSURE CASE CoMpAtAbIlItY\n",
    "positive_words = lm_dict.filter(lm_dict[\"Positive\"] > 0).to_series().str.to_lowercase()\n",
    "\n",
    "negative_words = lm_dict.filter(lm_dict[\"Negative\"] > 0).to_series().str.to_lowercase()\n",
    "print(len(negative_words) + len(positive_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (74_024, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2019-05-09</th><th>2019-08-08</th><th>2019-11-07</th><th>2020-02-27</th><th>2020-05-07</th><th>2020-08-06</th><th>2020-11-05</th><th>2021-02-25</th><th>2021-05-13</th><th>2021-08-12</th><th>2021-11-12</th><th>2022-02-24</th><th>2022-05-11</th><th>2022-08-10</th><th>2022-11-09</th><th>2023-02-23</th><th>2023-05-10</th><th>2023-08-09</th><th>2023-11-08</th><th>2024-02-22</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Document&quot;</td><td>&quot;Document&quot;</td><td>&quot;Document&quot;</td><td>&quot;Document&quot;</td><td>&quot;Document&quot;</td><td>&quot;Document&quot;</td><td>&quot;STATESSECURITI…</td><td>&quot;For&quot;</td><td>&quot;STATESSECURITI…</td><td>&quot;STATESSECURITI…</td><td>&quot;STATESSECURITI…</td><td>&quot;For&quot;</td><td>&quot;STATESSECURITI…</td><td>&quot;STATESSECURITI…</td><td>&quot;STATESSECURITI…</td><td>&quot;For&quot;</td><td>&quot;STATESSECURITI…</td><td>&quot;STATESSECURITI…</td><td>&quot;STATESSECURITI…</td><td>&quot;For&quot;</td></tr><tr><td>&quot;UNITED&quot;</td><td>&quot;usgaapCommonSt…</td><td>&quot;usgaapCommonSt…</td><td>&quot;usgaapCommonSt…</td><td>&quot;usgaapCommonSt…</td><td>&quot;usgaapCommonSt…</td><td>&quot;AND&quot;</td><td>&quot;the&quot;</td><td>&quot;AND&quot;</td><td>&quot;AND&quot;</td><td>&quot;AND&quot;</td><td>&quot;the&quot;</td><td>&quot;AND&quot;</td><td>&quot;AND&quot;</td><td>&quot;AND&quot;</td><td>&quot;the&quot;</td><td>&quot;AND&quot;</td><td>&quot;AND&quot;</td><td>&quot;AND&quot;</td><td>&quot;the&quot;</td></tr><tr><td>&quot;STATESSECURITI…</td><td>&quot;usgaapParentMe…</td><td>&quot;usgaapParentMe…</td><td>&quot;pfeGSKConsumer…</td><td>&quot;pfeGSKConsumer…</td><td>&quot;pfeGSKConsumer…</td><td>&quot;EXCHANGE&quot;</td><td>&quot;fiscal&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;fiscal&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;fiscal&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;EXCHANGE&quot;</td><td>&quot;fiscal&quot;</td></tr><tr><td>&quot;AND&quot;</td><td>&quot;usgaapPreferre…</td><td>&quot;usgaapAddition…</td><td>&quot;pfeLaboratorio…</td><td>&quot;usgaapAddition…</td><td>&quot;pfeGSKConsumer…</td><td>&quot;COMMISSIONWASH…</td><td>&quot;year&quot;</td><td>&quot;COMMISSIONWASH…</td><td>&quot;COMMISSIONWASH…</td><td>&quot;COMMISSIONWASH…</td><td>&quot;year&quot;</td><td>&quot;COMMISSIONWASH…</td><td>&quot;COMMISSIONWASH…</td><td>&quot;COMMISSIONWASH…</td><td>&quot;year&quot;</td><td>&quot;COMMISSIONWASH…</td><td>&quot;COMMISSIONWASH…</td><td>&quot;COMMISSIONWASH…</td><td>&quot;year&quot;</td></tr><tr><td>&quot;EXCHANGE&quot;</td><td>&quot;usgaapRetained…</td><td>&quot;usgaapAddition…</td><td>&quot;pfeHisunPfizer…</td><td>&quot;usgaapNoncontr…</td><td>&quot;usgaapPreferre…</td><td>&quot;DC&quot;</td><td>&quot;ended&quot;</td><td>&quot;DC&quot;</td><td>&quot;DC&quot;</td><td>&quot;DC&quot;</td><td>&quot;ended&quot;</td><td>&quot;DC&quot;</td><td>&quot;DC&quot;</td><td>&quot;DC&quot;</td><td>&quot;ended&quot;</td><td>&quot;DC&quot;</td><td>&quot;DC&quot;</td><td>&quot;DC&quot;</td><td>&quot;ended&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;and&quot;</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;on&quot;</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;the&quot;</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;date&quot;</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;indicated&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (74_024, 20)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ 2019-05-0 ┆ 2019-08-0 ┆ 2019-11-0 ┆ 2020-02-2 ┆ … ┆ 2023-05-1 ┆ 2023-08-0 ┆ 2023-11-0 ┆ 2024-02- │\n",
       "│ 9         ┆ 8         ┆ 7         ┆ 7         ┆   ┆ 0         ┆ 9         ┆ 8         ┆ 22       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ str       ┆ str       ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ Document  ┆ Document  ┆ Document  ┆ Document  ┆ … ┆ STATESSEC ┆ STATESSEC ┆ STATESSEC ┆ For      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ URITIES   ┆ URITIES   ┆ URITIES   ┆          │\n",
       "│ UNITED    ┆ usgaapCom ┆ usgaapCom ┆ usgaapCom ┆ … ┆ AND       ┆ AND       ┆ AND       ┆ the      │\n",
       "│           ┆ monStockM ┆ monStockM ┆ monStockM ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ ember     ┆ ember     ┆ ember     ┆   ┆           ┆           ┆           ┆          │\n",
       "│ STATESSEC ┆ usgaapPar ┆ usgaapPar ┆ pfeGSKCon ┆ … ┆ EXCHANGE  ┆ EXCHANGE  ┆ EXCHANGE  ┆ fiscal   │\n",
       "│ URITIES   ┆ entMember ┆ entMember ┆ sumerHeal ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ thcareMem ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆           ┆ ber       ┆   ┆           ┆           ┆           ┆          │\n",
       "│ AND       ┆ usgaapPre ┆ usgaapAdd ┆ pfeLabora ┆ … ┆ COMMISSIO ┆ COMMISSIO ┆ COMMISSIO ┆ year     │\n",
       "│           ┆ ferredSto ┆ itionalPa ┆ torioTeut ┆   ┆ NWASHINGT ┆ NWASHINGT ┆ NWASHINGT ┆          │\n",
       "│           ┆ ckMember  ┆ idInCapit ┆ oBrasiler ┆   ┆ ON        ┆ ON        ┆ ON        ┆          │\n",
       "│           ┆           ┆ alMem…    ┆ oMemb…    ┆   ┆           ┆           ┆           ┆          │\n",
       "│ EXCHANGE  ┆ usgaapRet ┆ usgaapAdd ┆ pfeHisunP ┆ … ┆ DC        ┆ DC        ┆ DC        ┆ ended    │\n",
       "│           ┆ ainedEarn ┆ itionalPa ┆ fizerPhar ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ ingsMembe ┆ idInCapit ┆ maceutica ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ r         ┆ alMem…    ┆ lsCoL…    ┆   ┆           ┆           ┆           ┆          │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ null      ┆ null      ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ and      │\n",
       "│ null      ┆ null      ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ on       │\n",
       "│ null      ┆ null      ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ the      │\n",
       "│ null      ┆ null      ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ date     │\n",
       "│ null      ┆ null      ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ indicate │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ d        │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example series \n",
    "example_series = df_dict['df_3M'][df_dict['df_3M'].columns[1]]\n",
    "example_df = df_dict['df_PFIZER']\n",
    "example_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_Apple (27983, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_MICROSOFT (40995, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_BigCommerce (50648, 15)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_ROKU (58304, 21)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_JPMORGAN (136587, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_VISA (50837, 21)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_Block (74461, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_Robinhood (77340, 11)\n",
      "[String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_JOHNSON (55611, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_PFIZER (74024, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_Moderna (147655, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_Teladoc (62377, 21)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_EXXON (40099, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_CHEVRON (47748, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_FIRST (68060, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_PLUG (138207, 22)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_GENERAL (61709, 21)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_3M (59572, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_CATERPILLAR (47425, 20)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n",
      "df_FASTENAL (35789, 21)\n",
      "[String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String, String]\n"
     ]
    }
   ],
   "source": [
    "#for key in list(df_dict.keys()):\n",
    "#    if df_dict[key].shape[0] == 0:\n",
    "#        del df_dict[key]\n",
    "#\n",
    "for key, value in df_dict.items():\n",
    "    print(key, value.shape)  \n",
    "    print(value.dtypes)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the func, that appled to a series and get sentiment words from it. As an output we get a df with first col - grouped words that appeaps in LOUGHRAN dictionary and has >0 value in \"positive\" or \"negative\" cols. Secon col contain count of corresponding word in this document. Third col contain boolean, indicatin if this word has positive sentiment (True) or negative (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (226, 3)\n",
      "┌─────────────┬──────────────┬──────────┐\n",
      "│ 2019-07-26  ┆ count in doc ┆ positive │\n",
      "│ ---         ┆ ---          ┆ ---      │\n",
      "│ str         ┆ u32          ┆ bool     │\n",
      "╞═════════════╪══════════════╪══════════╡\n",
      "│ retaliatory ┆ 1            ┆ false    │\n",
      "│ serious     ┆ 3            ┆ false    │\n",
      "│ forfeited   ┆ 1            ┆ false    │\n",
      "│ breach      ┆ 1            ┆ false    │\n",
      "│ denied      ┆ 3            ┆ false    │\n",
      "│ …           ┆ …            ┆ …        │\n",
      "│ innovative  ┆ 1            ┆ true     │\n",
      "│ achieve     ┆ 1            ┆ true     │\n",
      "│ enable      ┆ 7            ┆ true     │\n",
      "│ strength    ┆ 4            ┆ true     │\n",
      "│ enjoyment   ┆ 2            ┆ true     │\n",
      "└─────────────┴──────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "def count_sentiment_w_in_doc(df_series):\n",
    "        \n",
    "        #grouped series that became a df, where each row is unique word and its count in document \n",
    "        num_of_w = df_series.value_counts().rename({\"count\": \"count in doc\"})\n",
    "       \n",
    "        \n",
    "        #first col of this df (only words by itself)\n",
    "        only_w = num_of_w[num_of_w.columns[0]]\n",
    "        \n",
    "\n",
    "        #Filterd df, there is only words that have positive or negative sentiment score in this document \n",
    "        pos_w = num_of_w.filter(only_w.is_in(positive_words)).with_columns(pl.lit(True).alias(\"positive\"))\n",
    "        neg_w = num_of_w.filter(only_w.is_in(negative_words)).with_columns(pl.lit(False).alias(\"positive\"))\n",
    "       \n",
    "        #Staked for one df for further operations \n",
    "        df_stacked = neg_w.vstack(pos_w)\n",
    "\n",
    "        return df_stacked\n",
    "    \n",
    "check = count_sentiment_w_in_doc(df_dict['df_3M'][df_dict['df_3M'].columns[1]])  \n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the func that counts a number of docs with at least one occurense of the input word. It takes whole dictionary whith dataframes and particular word as an input. Then checks how many cols in each df contain this word, sum up all resulst and return a single value for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_all_doc_occurence(check_str, df_dict):\n",
    "    count = 0\n",
    "    for df in df_dict.values():\n",
    "        count += df.select((pl.all() == check_str).sum() > 0).sum_horizontal()[0]\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringg = \"a\"\n",
    "in_all_doc_occurence(stringg, df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the func that takes a df from count_sentiment_w_in_doc() and for each word in this df applies in_all_doc_occurence(), meaning that it counts all docs with at least one occurence of this word. The result of applying this func for each word in df is a series of integers showing counts for correcdonding word. This series augmented af col named \"count in all docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_occurence_count(df_series, df_dict):\n",
    "   check = count_sentiment_w_in_doc(df_series).lazy()\n",
    "\n",
    "   counts = check.with_columns(pl.col(check.columns[0]).map_elements(lambda x: in_all_doc_occurence(x, df_dict)).alias(\"count in all docs\")).collect()\n",
    "   return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (226, 4)\n",
      "┌─────────────────┬──────────────┬──────────┬───────────────────┐\n",
      "│ 2019-07-26      ┆ count in doc ┆ positive ┆ count in all docs │\n",
      "│ ---             ┆ ---          ┆ ---      ┆ ---               │\n",
      "│ str             ┆ u32          ┆ bool     ┆ i64               │\n",
      "╞═════════════════╪══════════════╪══════════╪═══════════════════╡\n",
      "│ complaint       ┆ 20           ┆ false    ┆ 253               │\n",
      "│ vulnerabilities ┆ 2            ┆ false    ┆ 123               │\n",
      "│ contended       ┆ 1            ┆ false    ┆ 9                 │\n",
      "│ disclosed       ┆ 6            ┆ false    ┆ 378               │\n",
      "│ improper        ┆ 3            ┆ false    ┆ 178               │\n",
      "│ …               ┆ …            ┆ …        ┆ …                 │\n",
      "│ efficient       ┆ 2            ┆ true     ┆ 195               │\n",
      "│ success         ┆ 2            ┆ true     ┆ 259               │\n",
      "│ better          ┆ 8            ┆ true     ┆ 273               │\n",
      "│ enabled         ┆ 1            ┆ true     ┆ 103               │\n",
      "│ innovator       ┆ 1            ┆ true     ┆ 40                │\n",
      "└─────────────────┴──────────────┴──────────┴───────────────────┘\n"
     ]
    }
   ],
   "source": [
    "res_df = total_occurence_count(example_series, df_dict)\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the func that takes as an argument resulting df, wich was obtained from total_occurence_count(). Recal that this df contains information about one document (one col in df). This func calculate weighted and raw scores using precomputed metrics. Returns a list with weighted score and raw score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(res_df):\n",
    "\n",
    "    res_df.lazy()\n",
    "\n",
    "    res = res_df.with_columns((((1 + pl.col(res_df.columns[1]).log())/(1 + ln_average_word_count))*((total_num_of_rep/pl.col(res_df.columns[3])).log())).alias(\"metrics\"))\n",
    "\n",
    "    w_pos_sen = res.filter(pl.col('positive') == True).select(pl.col(res.columns[4])).sum().item()\n",
    "    w_neg_sen = res.filter(pl.col('positive') == False).select(pl.col(res.columns[4])).sum().item()\n",
    "\n",
    "    w_score = w_pos_sen - w_neg_sen\n",
    "    score = res.select(pl.col('positive')).sum().item()*2 - len(res)\n",
    "\n",
    "    scores = [w_score, score]\n",
    "    \n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-25.89299863065233, -144]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_pysentiment(series):\n",
    "    sentiment = ...\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General loop that iterates over dfs and over it columns. It uses predefined finctions to calculate scores and save it as parquet files. Each parquet file named with company name is a dataframe with col names as field dates, first row is weighted scores and second row is raw scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "PanicException",
     "evalue": "python function failed KeyboardInterrupt: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\310\\Desktop\\Progects_Py\\env\\Lib\\site-packages\\polars\\expr\\expr.py:4049\u001b[0m, in \u001b[0;36mExpr._map_batches_wrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m-> 4049\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4050\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _check_for_numpy(result) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4051\u001b[0m         result \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mSeries(result, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_dtype)\n",
      "File \u001b[1;32mc:\\Users\\310\\Desktop\\Progects_Py\\env\\Lib\\site-packages\\polars\\expr\\expr.py:4384\u001b[0m, in \u001b[0;36mExpr.map_elements.<locals>.wrap_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   4382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m   4383\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, PolarsInefficientMapWarning)\n\u001b[1;32m-> 4384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_elements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_nulls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_nulls\u001b[49m\n\u001b[0;32m   4386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\310\\Desktop\\Progects_Py\\env\\Lib\\site-packages\\polars\\series\\series.py:5301\u001b[0m, in \u001b[0;36mSeries.map_elements\u001b[1;34m(self, function, return_dtype, skip_nulls)\u001b[0m\n\u001b[0;32m   5297\u001b[0m     pl_return_dtype \u001b[38;5;241m=\u001b[39m py_type_to_dtype(return_dtype)\n\u001b[0;32m   5299\u001b[0m warn_on_inefficient_map(function, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname], map_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pyseries(\n\u001b[1;32m-> 5301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_return_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_nulls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5302\u001b[0m )\n",
      "\u001b[1;31mPanicException\u001b[0m: python function failed KeyboardInterrupt: "
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "python function failed KeyboardInterrupt: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m score_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df:\n\u001b[1;32m----> 7\u001b[0m     res_df \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_occurence_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     scores \u001b[38;5;241m=\u001b[39m compute_score(res_df)\n\u001b[0;32m     10\u001b[0m     score_dict[col\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m  scores\n",
      "Cell \u001b[1;32mIn[75], line 4\u001b[0m, in \u001b[0;36mtotal_occurence_count\u001b[1;34m(df_series, df_dict)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_occurence_count\u001b[39m(df_series, df_dict):\n\u001b[0;32m      2\u001b[0m    check \u001b[38;5;241m=\u001b[39m count_sentiment_w_in_doc(df_series)\u001b[38;5;241m.\u001b[39mlazy()\n\u001b[1;32m----> 4\u001b[0m    counts \u001b[38;5;241m=\u001b[39m \u001b[43mcheck\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43min_all_doc_occurence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount in all docs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m counts\n",
      "File \u001b[1;32mc:\\Users\\310\\Desktop\\Progects_Py\\env\\Lib\\site-packages\\polars\\lazyframe\\frame.py:1939\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\u001b[0m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m background:\n\u001b[0;32m   1937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InProcessQuery(ldf\u001b[38;5;241m.\u001b[39mcollect_concurrently())\n\u001b[1;32m-> 1939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(ldf\u001b[38;5;241m.\u001b[39mcollect())\n",
      "\u001b[1;31mPanicException\u001b[0m: python function failed KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "\n",
    "for df in df_dict.values():\n",
    "\n",
    "    score_dict = {}\n",
    "\n",
    "    for col in df:\n",
    "        \n",
    "        res_df = total_occurence_count(col, df_dict)\n",
    "        scores = compute_score(res_df)\n",
    "\n",
    "        scores.append(compute_pysentiment(col))\n",
    "\n",
    "        score_dict[col.name] =  scores\n",
    "    \n",
    "    df = pl.DataFrame(score_dict)\n",
    "    print(df)\n",
    "    \n",
    "    print(f'Iteration for {col[0]} is done successfully')\n",
    "\n",
    " # Determine the output directory and file name\n",
    "    output_dir = os.path.join('.', 'full_10_Q_scores')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    file_name_new = f\"{col[0]}_f_10_Q_score_df.parquet\"\n",
    "    full_path = os.path.join(output_dir, file_name_new)\n",
    "    full_path = os.path.normpath(full_path)\n",
    "\n",
    "    print(f\"Attempting to write to: {full_path}\")\n",
    "\n",
    "    # Write the DataFrame to Parquet\n",
    "    df.write_parquet(full_path)\n",
    "\n",
    "    print(f\"{full_path} created successfully\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
