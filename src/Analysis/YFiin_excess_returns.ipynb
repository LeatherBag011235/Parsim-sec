{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import math \n",
    "import pyarrow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME_LIST = ['Apple%2520Inc.%2520(AAPL)%2520(CIK%25200000320193)', 'MICROSOFT%2520CORP%2520(MSFT)%2520(CIK%25200000789019)', \n",
    "                     'BigCommerce%2520Holdings%252C%2520Inc.%2520(BIGC)%2520(CIK%25200001626450)', 'ROKU%252C%2520INC%2520(ROKU)%2520(CIK%25200001428439)', \n",
    "                     'JPMORGAN%2520CHASE%2520%2526%2520CO%2520(JPM%252C%2520AMJ%252C%2520AMJB%252C%2520JPM)%2520(CIK%25200000019617)', 'VISA%2520INC.%2520(V)%2520(CIK%25200001403161)', \n",
    "                     'Block%252C%2520Inc.%2520(SQ%252C%2520BSQKZ)%2520(CIK%25200001512673)', 'Robinhood%2520Markets%252C%2520Inc.%2520(HOOD)%2520(CIK%25200001783879)', \n",
    "                     'JOHNSON%252C%252C%2520JOHNSON%2520(JNJ)%2520(CIK%25200000200406)', 'PFIZER%2520INC%2520(PFE)%2520(CIK%25200000078003)', \n",
    "                     'Moderna%252C%2520Inc.%2520(MRNA)%2520(CIK%25200001682852)', 'Teladoc%2520Health%252C%2520Inc.%2520(TDOC)%2520(CIK%25200001477449)', \n",
    "                     'EXXON%2520MOBIL%2520CORP%2520(XOM)%2520(CIK%25200000034088)', 'CHEVRON%2520CORP%2520(CVX)%2520(CIK%25200000093410)', \n",
    "                     'FIRST%2520SOLAR%252C%2520INC.%2520(FSLR)%2520(CIK%25200001274494)', 'PLUG%2520POWER%2520INC%2520(PLUG)%2520(CIK%25200001093691)', \n",
    "                     'GENERAL%252CELECTRIC%2520CO%2520(GE)%2520(CIK%25200000040545)', '3M%2520CO%2520(MMM)%2520(CIK%25200000066740)', \n",
    "                     'CATERPILLAR%2520INC%2520(CAT)%2520(CIK%25200000018230)', 'FASTENAL%2520CO%2520(FAST)%2520(CIK%25200000815556)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect scores from hard memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_scores = {}\n",
    "regex = r\"\\\\([^\\\\]+)_full_doc_sen_score_df\\.parquet$\"\n",
    "\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\310\\Desktop\\Progects_Py\\Parsim-sec\\src\\Analysis\\data\\full_10_Q_scores'):\n",
    "    for file in files:\n",
    "            \n",
    "            file_path = os.path.join(root, file)\n",
    "            ticker = re.search(regex, file_path).group(1)\n",
    "            \n",
    "            df = pl.read_parquet(file_path)\n",
    "\n",
    "            df_dict_scores[ticker] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2019-04-30</th><th>2019-07-31</th><th>2019-10-30</th><th>2020-02-24</th><th>2020-04-29</th><th>2020-07-29</th><th>2020-10-28</th><th>2021-02-12</th><th>2021-04-27</th><th>2021-07-27</th><th>2021-10-26</th><th>2022-02-11</th><th>2022-04-26</th><th>2022-07-26</th><th>2022-10-25</th><th>2023-02-10</th><th>2023-04-25</th><th>2023-07-25</th><th>2023-10-24</th><th>2024-02-02</th><th>2024-04-23</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>-12.849008</td><td>-10.494805</td><td>-14.107974</td><td>-30.133511</td><td>-13.439952</td><td>-12.999193</td><td>-14.971735</td><td>-35.735787</td><td>-10.841224</td><td>-10.056447</td><td>-8.511603</td><td>-30.035251</td><td>-3.151193</td><td>-2.180779</td><td>-4.783131</td><td>-26.817862</td><td>-4.257227</td><td>-5.295587</td><td>-3.992991</td><td>-29.066429</td><td>-20.433151</td></tr><tr><td>-84.0</td><td>-68.0</td><td>-77.0</td><td>-168.0</td><td>-81.0</td><td>-73.0</td><td>-84.0</td><td>-195.0</td><td>-73.0</td><td>-65.0</td><td>-55.0</td><td>-169.0</td><td>-40.0</td><td>-28.0</td><td>-38.0</td><td>-160.0</td><td>-35.0</td><td>-43.0</td><td>-38.0</td><td>-161.0</td><td>-121.0</td></tr><tr><td>-0.367816</td><td>-0.335992</td><td>-0.348473</td><td>-0.29972</td><td>-0.366922</td><td>-0.363445</td><td>-0.350427</td><td>-0.325886</td><td>-0.354839</td><td>-0.307278</td><td>-0.264813</td><td>-0.282203</td><td>-0.256506</td><td>-0.23964</td><td>-0.295455</td><td>-0.298453</td><td>-0.230088</td><td>-0.289176</td><td>-0.244094</td><td>-0.27239</td><td>-0.361179</td></tr><tr><td>0.457193</td><td>0.434381</td><td>0.430875</td><td>0.445714</td><td>0.463025</td><td>0.404468</td><td>0.414253</td><td>0.446668</td><td>0.456579</td><td>0.441176</td><td>0.443396</td><td>0.439415</td><td>0.40296</td><td>0.387564</td><td>0.37332</td><td>0.429854</td><td>0.469191</td><td>0.416539</td><td>0.409936</td><td>0.437905</td><td>0.408483</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ 2019-04-3 ┆ 2019-07-3 ┆ 2019-10-3 ┆ 2020-02-2 ┆ … ┆ 2023-07-2 ┆ 2023-10-2 ┆ 2024-02-0 ┆ 2024-04- │\n",
       "│ 0         ┆ 1         ┆ 0         ┆ 4         ┆   ┆ 5         ┆ 4         ┆ 2         ┆ 23       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ -12.84900 ┆ -10.49480 ┆ -14.10797 ┆ -30.13351 ┆ … ┆ -5.295587 ┆ -3.992991 ┆ -29.06642 ┆ -20.4331 │\n",
       "│ 8         ┆ 5         ┆ 4         ┆ 1         ┆   ┆           ┆           ┆ 9         ┆ 51       │\n",
       "│ -84.0     ┆ -68.0     ┆ -77.0     ┆ -168.0    ┆ … ┆ -43.0     ┆ -38.0     ┆ -161.0    ┆ -121.0   │\n",
       "│ -0.367816 ┆ -0.335992 ┆ -0.348473 ┆ -0.29972  ┆ … ┆ -0.289176 ┆ -0.244094 ┆ -0.27239  ┆ -0.36117 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
       "│ 0.457193  ┆ 0.434381  ┆ 0.430875  ┆ 0.445714  ┆ … ┆ 0.416539  ┆ 0.409936  ┆ 0.437905  ┆ 0.408483 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict_scores[\"GE\"]\n",
    "tic_sym = \"GE\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is support func to find end prices and their dates. It takes as an arguments starting index, which is the nuber of row of hist dataframe and this hist dataframe itself. Hist dataframe contain daily OHLC data for particular ticker (company). \n",
    "\n",
    "It also cheks wether index of end price is in hist, meaning that if this particular report was released very resently (less then 8 trading days ago) it will append none values for end prices out of range of hist df instead of raising an error.\n",
    "\n",
    "It returns lists of end prices and corresponding dates (none values for prices and their dates, if they are out of range of hist df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_price(start_index, hist):\n",
    "    end_price_list = []\n",
    "    end_price_date_list = []\n",
    "    \n",
    "    for x in range(2, 8):\n",
    "        idx = start_index + x\n",
    "        if idx < len(hist):\n",
    "            end_price_list.append(hist.iloc[idx]['Open'])\n",
    "            end_price_date_list.append(hist.index[idx])\n",
    "        else:\n",
    "            end_price_list.append(None)\n",
    "            end_price_date_list.append(None)\n",
    "    return end_price_list, end_price_date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is support func to calc the regular returns. It takes start price and end_price_list as an arguments. It checks whether value in end price list is none and if it is, it appends none in returns list, instead of raising an error. So we are shure that all end price lists are of the same length, but for those prices that are not found in historical data we have none values for returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_returns(start_price, end_price_list):\n",
    "    retuns = []\n",
    "    for end_price in end_price_list:\n",
    "        if end_price is None:\n",
    "            retuns.append(None)\n",
    "        else:\n",
    "            ret = (end_price - start_price) / start_price * 100\n",
    "            retuns.append(ret)\n",
    "    return retuns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the support func that calc SnP500 returns. It takes_start date and end_price_date_list as and arguments and then calculate returns for this timeframes. This func also checks whether value of end_price_date_list is none (meaning that it is not found in historical data), and if it is, func will append none instead of raising an error.\n",
    "\n",
    "Now we are shure that list of snp_returns will be of equal length for all report release dates and all time frames, but for those returns that cannot be calculates due to non existent end prices we will have nones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snp_500_return(start_date, end_price_date_list, snp_price):\n",
    "\n",
    "    snp_returns = []\n",
    "\n",
    "    for end_date in end_price_date_list:\n",
    "        if end_date is None:\n",
    "            snp_returns.append(None)\n",
    "\n",
    "        else: \n",
    "            start_index = snp_price.index.get_loc(start_date)\n",
    "            start_price = snp_price.iloc[start_index]['Open']\n",
    "    \n",
    "            end_index = snp_price.index.get_loc(end_date)\n",
    "            end_price = snp_price.iloc[end_index]['Open']\n",
    "    \n",
    "            ret = (end_price - start_price) / start_price * 100\n",
    "            \n",
    "            snp_returns.append(ret)\n",
    "\n",
    "    return snp_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is suppurt function that retrive the end quarter price. It take as an argument hist - historical price dataframe, date_str - the date of release of this particular report, df - company dataframe, with cols named with dates of releases of corresponding reports.\n",
    "\n",
    "It takes the release date for current report and checks whether it is the last date in time series or not. If it is (this mean that we are are realy close to the end of hist df) we consider the end of hist df as the end of quarter. if it is not, we take next date in company df (which is the date of releas of the next quarterly report) and see its open price, this is pricisely the end of the quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_quarter(hist, date_str, df):\n",
    "\n",
    "    col_names = df.columns\n",
    "    current_date_index = col_names.index(date_str)\n",
    "\n",
    "    if current_date_index < len(col_names) - 1:\n",
    "\n",
    "        next_date = col_names[current_date_index + 1]\n",
    "\n",
    "        next_date_time_stemp = pd.Timestamp(next_date, tz='America/New_York')\n",
    "\n",
    "        while next_date_time_stemp not in hist.index:\n",
    "\n",
    "            next_date_time_stemp += pd.Timedelta(days=1)\n",
    "    \n",
    "        end_quarter_index = hist.index.get_loc(next_date_time_stemp)\n",
    "        end_quarter_price = hist.iloc[end_quarter_index]['Open']\n",
    "        end_quarter_date = hist.index[end_quarter_index]\n",
    "        \n",
    "        current_date_time_stemp = pd.Timestamp(date_str, tz='America/New_York')\n",
    "\n",
    "        if current_date_time_stemp in hist.index:\n",
    "\n",
    "            start_quarter_index = hist.index.get_loc(current_date_time_stemp)\n",
    "        else:\n",
    "            print('cannot find in index')\n",
    "            print(f'current_date_time_stemp: {current_date_time_stemp}')\n",
    "            first_date = hist.index[0]\n",
    "            print(f'firts date: {first_date}')\n",
    "\n",
    "        quarter_length = end_quarter_index - start_quarter_index\n",
    "\n",
    "    else:\n",
    "        end_quarter_price = None\n",
    "        end_quarter_date = None\n",
    "        quarter_length = None\n",
    "    \n",
    "\n",
    "    return end_quarter_price, end_quarter_date, quarter_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a func that takes as an input a key-value pair from df_dict_scores and calculate 2-days, 3-days, 4-days, 5-days, 6-days, 7-days and full quarter excess returns for it (S&P500 is the benchmark). It stores the result as a polars df with dates of starting of the period as col names and corresponding returns as col values (seven in each: starting from 2-days down to 7-days and full quarter) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-04 00:00:00-04:00</th>\n",
       "      <td>63.168677</td>\n",
       "      <td>64.352804</td>\n",
       "      <td>62.713239</td>\n",
       "      <td>64.216171</td>\n",
       "      <td>8744820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07 00:00:00-04:00</th>\n",
       "      <td>64.170630</td>\n",
       "      <td>64.580521</td>\n",
       "      <td>63.760739</td>\n",
       "      <td>64.079544</td>\n",
       "      <td>6858410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08 00:00:00-04:00</th>\n",
       "      <td>64.580549</td>\n",
       "      <td>65.992396</td>\n",
       "      <td>64.170658</td>\n",
       "      <td>64.990440</td>\n",
       "      <td>12381195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09 00:00:00-04:00</th>\n",
       "      <td>65.354776</td>\n",
       "      <td>66.857708</td>\n",
       "      <td>65.127057</td>\n",
       "      <td>66.584450</td>\n",
       "      <td>10614202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10 00:00:00-04:00</th>\n",
       "      <td>67.222042</td>\n",
       "      <td>67.222042</td>\n",
       "      <td>66.037914</td>\n",
       "      <td>66.903236</td>\n",
       "      <td>7820843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-29 00:00:00-04:00</th>\n",
       "      <td>163.000000</td>\n",
       "      <td>166.070007</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>164.490005</td>\n",
       "      <td>5371700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-30 00:00:00-04:00</th>\n",
       "      <td>163.559998</td>\n",
       "      <td>166.259995</td>\n",
       "      <td>161.580002</td>\n",
       "      <td>161.820007</td>\n",
       "      <td>6723600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-01 00:00:00-04:00</th>\n",
       "      <td>162.570007</td>\n",
       "      <td>163.300003</td>\n",
       "      <td>158.820007</td>\n",
       "      <td>159.699997</td>\n",
       "      <td>4775800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-02 00:00:00-04:00</th>\n",
       "      <td>161.389999</td>\n",
       "      <td>162.820007</td>\n",
       "      <td>159.419998</td>\n",
       "      <td>162.639999</td>\n",
       "      <td>5030400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-03 00:00:00-04:00</th>\n",
       "      <td>165.300003</td>\n",
       "      <td>165.300003</td>\n",
       "      <td>162.010101</td>\n",
       "      <td>164.110001</td>\n",
       "      <td>3968626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1510 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close  \\\n",
       "Date                                                                        \n",
       "2018-05-04 00:00:00-04:00   63.168677   64.352804   62.713239   64.216171   \n",
       "2018-05-07 00:00:00-04:00   64.170630   64.580521   63.760739   64.079544   \n",
       "2018-05-08 00:00:00-04:00   64.580549   65.992396   64.170658   64.990440   \n",
       "2018-05-09 00:00:00-04:00   65.354776   66.857708   65.127057   66.584450   \n",
       "2018-05-10 00:00:00-04:00   67.222042   67.222042   66.037914   66.903236   \n",
       "...                               ...         ...         ...         ...   \n",
       "2024-04-29 00:00:00-04:00  163.000000  166.070007  163.000000  164.490005   \n",
       "2024-04-30 00:00:00-04:00  163.559998  166.259995  161.580002  161.820007   \n",
       "2024-05-01 00:00:00-04:00  162.570007  163.300003  158.820007  159.699997   \n",
       "2024-05-02 00:00:00-04:00  161.389999  162.820007  159.419998  162.639999   \n",
       "2024-05-03 00:00:00-04:00  165.300003  165.300003  162.010101  164.110001   \n",
       "\n",
       "                             Volume  Dividends  Stock Splits  \n",
       "Date                                                          \n",
       "2018-05-04 00:00:00-04:00   8744820        0.0           0.0  \n",
       "2018-05-07 00:00:00-04:00   6858410        0.0           0.0  \n",
       "2018-05-08 00:00:00-04:00  12381195        0.0           0.0  \n",
       "2018-05-09 00:00:00-04:00  10614202        0.0           0.0  \n",
       "2018-05-10 00:00:00-04:00   7820843        0.0           0.0  \n",
       "...                             ...        ...           ...  \n",
       "2024-04-29 00:00:00-04:00   5371700        0.0           0.0  \n",
       "2024-04-30 00:00:00-04:00   6723600        0.0           0.0  \n",
       "2024-05-01 00:00:00-04:00   4775800        0.0           0.0  \n",
       "2024-05-02 00:00:00-04:00   5030400        0.0           0.0  \n",
       "2024-05-03 00:00:00-04:00   3968626        0.0           0.0  \n",
       "\n",
       "[1510 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_ticker = yf.Ticker(tic_sym)\n",
    "hist = company_ticker.history(period=\"6y\")\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_returns(tic_sym, df):\n",
    "\n",
    "    company_ticker = yf.Ticker(tic_sym)\n",
    "    hist = company_ticker.history(period=\"6y\")\n",
    "\n",
    "    snp500_ticker = yf.Ticker(\"^GSPC\")\n",
    "    snp_price = snp500_ticker.history(period=\"6y\", auto_adjust=True)\n",
    "    \n",
    "    returns = {}\n",
    "\n",
    "    for date_str in df.columns:\n",
    "\n",
    "        start_date = pd.Timestamp(date_str, tz='America/New_York')\n",
    "        \n",
    "        while start_date not in hist.index:\n",
    "\n",
    "            start_date += pd.Timedelta(days=1)\n",
    "        \n",
    "\n",
    "        start_index = hist.index.get_loc(start_date)\n",
    "\n",
    "        start_price = hist.iloc[start_index]['Open']\n",
    "\n",
    "        end_price_list, end_price_date_list = find_end_price(start_index, hist)\n",
    "        \n",
    "        if None not in end_price_list:\n",
    "            end_quarter_price, end_quarter_date, length_of_quarter = find_end_quarter(hist, date_str, df)\n",
    "            \n",
    "        else:\n",
    "            end_quarter_price = None\n",
    "            end_quarter_date = None\n",
    "\n",
    "        \n",
    "        end_price_list.append(end_quarter_price)\n",
    "        end_price_date_list.append(end_quarter_date)\n",
    "        \n",
    "        reg_returns = regular_returns(start_price, end_price_list)\n",
    "        snp_returns = snp_500_return(start_date, end_price_date_list, snp_price)\n",
    "        \n",
    "        excess_returns = [a - b if a is not None and b is not None else None for a, b in zip(reg_returns, snp_returns)]\n",
    "        \n",
    "        #normalization: divide returns for each time frame by number of trading days (2-days returns/2, 3-days returns/3, ful quarter returns/length_of_quarter)\n",
    "        timeframe_length = [2, 3, 4, 5, 6, 7, length_of_quarter]\n",
    "\n",
    "        normalized_excess_returns = [x / y if x is not None and y is not None else None for x, y in zip(excess_returns, timeframe_length)]\n",
    "\n",
    "        returns[date_str] = normalized_excess_returns\n",
    "\n",
    "    returns = pl.DataFrame(returns)\n",
    "    \n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2019-04-30</th><th>2019-07-31</th><th>2019-10-30</th><th>2020-02-24</th><th>2020-04-29</th><th>2020-07-29</th><th>2020-10-28</th><th>2021-02-12</th><th>2021-04-27</th><th>2021-07-27</th><th>2021-10-26</th><th>2022-02-11</th><th>2022-04-26</th><th>2022-07-26</th><th>2022-10-25</th><th>2023-02-10</th><th>2023-04-25</th><th>2023-07-25</th><th>2023-10-24</th><th>2024-02-02</th><th>2024-04-23</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>-0.135998</td><td>-2.286272</td><td>1.353142</td><td>0.438005</td><td>0.770967</td><td>-5.963048</td><td>-0.400486</td><td>1.796756</td><td>-1.398768</td><td>-0.380327</td><td>-0.947382</td><td>0.334166</td><td>-3.211746</td><td>2.572387</td><td>0.402788</td><td>0.106715</td><td>-1.798186</td><td>-0.009876</td><td>1.030062</td><td>0.483866</td><td>0.171553</td></tr><tr><td>0.318089</td><td>-1.699238</td><td>1.751244</td><td>-1.055001</td><td>-0.714642</td><td>-4.683881</td><td>0.417153</td><td>1.220333</td><td>-0.917904</td><td>-0.264633</td><td>-0.212925</td><td>0.733905</td><td>-2.737171</td><td>0.457403</td><td>0.673731</td><td>0.328125</td><td>-1.601779</td><td>0.222579</td><td>0.108764</td><td>0.398735</td><td>0.301743</td></tr><tr><td>0.264892</td><td>-1.15454</td><td>2.576421</td><td>-1.106406</td><td>-1.072294</td><td>-3.66883</td><td>0.512642</td><td>0.618453</td><td>-0.503753</td><td>-0.452485</td><td>-0.174338</td><td>0.614352</td><td>-2.333879</td><td>0.292274</td><td>0.384414</td><td>0.584644</td><td>-0.847759</td><td>-0.175717</td><td>-0.225727</td><td>0.19215</td><td>0.394966</td></tr><tr><td>0.379225</td><td>-1.408714</td><td>2.119082</td><td>0.91828</td><td>-1.195077</td><td>-2.904893</td><td>0.096531</td><td>1.290329</td><td>-0.19306</td><td>-1.100746</td><td>-0.041674</td><td>0.418953</td><td>-1.353677</td><td>0.831982</td><td>0.41057</td><td>0.646402</td><td>-0.445455</td><td>-0.24528</td><td>0.027005</td><td>0.17023</td><td>0.428098</td></tr><tr><td>0.082069</td><td>-1.273047</td><td>1.990255</td><td>0.197163</td><td>-1.317331</td><td>-2.0692</td><td>0.262141</td><td>1.970794</td><td>-0.375225</td><td>-0.753256</td><td>-0.126099</td><td>-0.432046</td><td>-0.948535</td><td>0.600967</td><td>0.452159</td><td>0.34218</td><td>0.005442</td><td>-0.207122</td><td>-0.276899</td><td>0.130259</td><td>0.499944</td></tr><tr><td>0.208398</td><td>-1.306744</td><td>1.922943</td><td>0.159189</td><td>-0.957994</td><td>-1.930071</td><td>0.261255</td><td>1.87889</td><td>-0.231115</td><td>-0.463295</td><td>-0.247973</td><td>-0.179654</td><td>-0.910938</td><td>0.257512</td><td>0.605062</td><td>0.474204</td><td>0.087456</td><td>-0.14207</td><td>-0.388662</td><td>0.313372</td><td>0.264078</td></tr><tr><td>0.040864</td><td>-0.152569</td><td>0.160232</td><td>-0.703805</td><td>-0.091146</td><td>0.063911</td><td>0.480515</td><td>0.227109</td><td>-0.104953</td><td>-0.068808</td><td>-0.062514</td><td>-0.181003</td><td>-0.165445</td><td>0.182692</td><td>0.420243</td><td>0.497866</td><td>0.032636</td><td>0.063846</td><td>0.086811</td><td>0.799817</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ 2019-04-3 ┆ 2019-07-3 ┆ 2019-10-3 ┆ 2020-02-2 ┆ … ┆ 2023-07-2 ┆ 2023-10-2 ┆ 2024-02-0 ┆ 2024-04- │\n",
       "│ 0         ┆ 1         ┆ 0         ┆ 4         ┆   ┆ 5         ┆ 4         ┆ 2         ┆ 23       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ -0.135998 ┆ -2.286272 ┆ 1.353142  ┆ 0.438005  ┆ … ┆ -0.009876 ┆ 1.030062  ┆ 0.483866  ┆ 0.171553 │\n",
       "│ 0.318089  ┆ -1.699238 ┆ 1.751244  ┆ -1.055001 ┆ … ┆ 0.222579  ┆ 0.108764  ┆ 0.398735  ┆ 0.301743 │\n",
       "│ 0.264892  ┆ -1.15454  ┆ 2.576421  ┆ -1.106406 ┆ … ┆ -0.175717 ┆ -0.225727 ┆ 0.19215   ┆ 0.394966 │\n",
       "│ 0.379225  ┆ -1.408714 ┆ 2.119082  ┆ 0.91828   ┆ … ┆ -0.24528  ┆ 0.027005  ┆ 0.17023   ┆ 0.428098 │\n",
       "│ 0.082069  ┆ -1.273047 ┆ 1.990255  ┆ 0.197163  ┆ … ┆ -0.207122 ┆ -0.276899 ┆ 0.130259  ┆ 0.499944 │\n",
       "│ 0.208398  ┆ -1.306744 ┆ 1.922943  ┆ 0.159189  ┆ … ┆ -0.14207  ┆ -0.388662 ┆ 0.313372  ┆ 0.264078 │\n",
       "│ 0.040864  ┆ -0.152569 ┆ 0.160232  ┆ -0.703805 ┆ … ┆ 0.063846  ┆ 0.086811  ┆ 0.799817  ┆ null     │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computing_returns(tic_sym, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2019-05-10</th><th>2019-08-09</th><th>2019-11-08</th><th>2020-03-02</th><th>2020-05-11</th><th>2020-08-07</th><th>2020-11-06</th><th>2021-02-26</th><th>2021-05-07</th><th>2021-08-05</th><th>2021-11-04</th><th>2022-02-18</th><th>2022-04-29</th><th>2022-07-29</th><th>2022-11-03</th><th>2023-02-16</th><th>2023-04-27</th><th>2023-07-28</th><th>2023-11-02</th><th>2024-02-16</th><th>2024-04-26</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>-27.295226</td><td>-26.769276</td><td>-27.194249</td><td>-28.559457</td><td>-33.156006</td><td>-34.871952</td><td>-34.746064</td><td>-32.90677</td><td>-36.729215</td><td>-36.477945</td><td>-36.595384</td><td>-30.815151</td><td>-34.560856</td><td>-37.275143</td><td>-37.896232</td><td>-33.891065</td><td>-38.898848</td><td>-38.186708</td><td>-38.393132</td><td>-38.267797</td><td>-39.034612</td></tr><tr><td>-164.0</td><td>-160.0</td><td>-162.0</td><td>-173.0</td><td>-192.0</td><td>-202.0</td><td>-200.0</td><td>-192.0</td><td>-201.0</td><td>-202.0</td><td>-200.0</td><td>-177.0</td><td>-184.0</td><td>-193.0</td><td>-196.0</td><td>-190.0</td><td>-200.0</td><td>-197.0</td><td>-198.0</td><td>-198.0</td><td>-200.0</td></tr><tr><td>-0.422897</td><td>-0.411494</td><td>-0.415051</td><td>-0.371429</td><td>-0.435664</td><td>-0.431111</td><td>-0.427043</td><td>-0.380845</td><td>-0.427751</td><td>-0.426284</td><td>-0.433662</td><td>-0.389239</td><td>-0.444867</td><td>-0.447124</td><td>-0.446593</td><td>-0.377617</td><td>-0.456204</td><td>-0.46217</td><td>-0.477837</td><td>-0.399584</td><td>-0.463652</td></tr><tr><td>0.466667</td><td>0.462373</td><td>0.469699</td><td>0.489944</td><td>0.45404</td><td>0.448459</td><td>0.447561</td><td>0.484229</td><td>0.448135</td><td>0.439471</td><td>0.425574</td><td>0.474099</td><td>0.413121</td><td>0.409879</td><td>0.409548</td><td>0.484827</td><td>0.407322</td><td>0.398859</td><td>0.387704</td><td>0.470352</td><td>0.404057</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ 2019-05-1 ┆ 2019-08-0 ┆ 2019-11-0 ┆ 2020-03-0 ┆ … ┆ 2023-07-2 ┆ 2023-11-0 ┆ 2024-02-1 ┆ 2024-04- │\n",
       "│ 0         ┆ 9         ┆ 8         ┆ 2         ┆   ┆ 8         ┆ 2         ┆ 6         ┆ 26       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ -27.29522 ┆ -26.76927 ┆ -27.19424 ┆ -28.55945 ┆ … ┆ -38.18670 ┆ -38.39313 ┆ -38.26779 ┆ -39.0346 │\n",
       "│ 6         ┆ 6         ┆ 9         ┆ 7         ┆   ┆ 8         ┆ 2         ┆ 7         ┆ 12       │\n",
       "│ -164.0    ┆ -160.0    ┆ -162.0    ┆ -173.0    ┆ … ┆ -197.0    ┆ -198.0    ┆ -198.0    ┆ -200.0   │\n",
       "│ -0.422897 ┆ -0.411494 ┆ -0.415051 ┆ -0.371429 ┆ … ┆ -0.46217  ┆ -0.477837 ┆ -0.399584 ┆ -0.46365 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 2        │\n",
       "│ 0.466667  ┆ 0.462373  ┆ 0.469699  ┆ 0.489944  ┆ … ┆ 0.398859  ┆ 0.387704  ┆ 0.470352  ┆ 0.404057 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict_scores[\"ROKU\"]\n",
    "tic_sym = \"ROKU\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the general loop, that takes each key-value pair from df_dict_scores, applies computing_returns() to it, then verticaly stackes the result with orig df and stores it in the new dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "BIGC\n",
      "CAT\n",
      "CVX\n",
      "FAST\n",
      "FSLR\n",
      "GE\n",
      "HOOD\n",
      "JNJ\n",
      "JPM\n",
      "MMM\n",
      "MRNA\n",
      "MSFT\n",
      "PFE\n",
      "PLUG\n",
      "ROKU\n",
      "SQ\n",
      "TDOC\n",
      "V\n",
      "XOM\n"
     ]
    }
   ],
   "source": [
    "dict_for_anal = {}\n",
    "row_names = pl.Series(\"row_names\", [\"weighted scores\", \"raw scores\", \"lm scores\", \"harvard scores\", \"2_day_reterns\",\"3_day_reterns\", \"4_day_reterns\", \"5_day_reterns\", \"6_day_reterns\", \"7_day_reterns\", \"full_quarter_returns\"])\n",
    "\n",
    "for tic_sym, df in df_dict_scores.items():\n",
    "    print(tic_sym)\n",
    "\n",
    "    returns_df = computing_returns(tic_sym, df)\n",
    "    \n",
    "    stacked_df = df.vstack(returns_df)\n",
    "    stacked_df_with_indx = stacked_df.hstack([row_names])\n",
    "    \n",
    "    dict_for_anal[tic_sym] = stacked_df_with_indx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start the analisys. The general question is: how well sentiment scores and weighted scores can explain returns? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see them on the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for company_name, scores in dict_for_anal.items():\n",
    "#   \n",
    "#    df_for_plt = scores.select(pl.exclude(\"row_names\"))\n",
    "#\n",
    "#    dates = [datetime.strptime(date, \"%Y-%m-%d\") for date in df_for_plt.columns]\n",
    "#   \n",
    "#    plt.figure(figsize=(10, 6))\n",
    "#    row_indices_to_plot = [0, 1, 4, -1]\n",
    "#\n",
    "#    for idx in row_indices_to_plot:\n",
    "#        row = df_for_plt.row(idx, named=True)  \n",
    "#        plt.plot(dates, list(row.values()), label=scores.row(idx)[-1])\n",
    "#\n",
    "#    plt.title(company_name)\n",
    "#    plt.xlabel(\"Date\")\n",
    "#    plt.ylabel(\"Values\")\n",
    "#    plt.legend()\n",
    "#    plt.xticks(rotation=45)\n",
    "#    plt.tight_layout()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance some time series exhibits at least piecewise dependence. \n",
    "\n",
    "Next step is to regress each return on weighted, raw scores and polarity scores from Loughran-McDonald and Harvard dictionaries.\n",
    "\n",
    "And also we can see that in cases of 10-K reports which are typicly much longer we can see strong pattern in raw scores which is smoothed in weighted scores. But still this pattern could be recognized. This can be interpreted as 10-K can contain more information about future prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This func takes as input key-value pair (company name and dataframe with sentiment scores and returns), regress each retern (2_days, 3_days, etc) on weightes sentiment score ans raw sentiment score. Then it collects the slopes (beta-coefficient) of returns regressed on raw score and on weighted score in the separate polars series (in the loop they will be staked in dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_quarter_dummy(company_df_pd):\n",
    "    \n",
    "    dates = pd.to_datetime(company_df_pd.columns)\n",
    "    date_series = pd.Series(dates.quarter, index=dates)\n",
    "\n",
    "    dummies = pd.get_dummies(date_series, prefix=\"Q\")\n",
    "    dummies.index = dummies.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    dummies = dummies.drop(columns=[\"Q_4\"])\n",
    "    \n",
    "    dummies = dummies.transpose()\n",
    "    df_wit_dum = pd.concat([company_df_pd, dummies])\n",
    "\n",
    "    df_wit_dum = df_wit_dum.transpose()\n",
    "\n",
    "    for col in df_wit_dum.columns:\n",
    "        df_wit_dum[col] = pd.to_numeric(df_wit_dum[col])\n",
    "\n",
    "    boolean_cols = ['Q_1', 'Q_2', 'Q_3']\n",
    "    df_wit_dum[boolean_cols] = df_wit_dum[boolean_cols].astype(int)\n",
    "\n",
    "    return df_wit_dum\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression(y, x):\n",
    "\n",
    "    model = sm.OLS(y, x, missing='drop').fit()\n",
    "    slope = model.params.iloc[1]\n",
    "    \n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_slopes(df_wit_dum, X_names, company_name=\"total_df\"):\n",
    "\n",
    "    X = [df_wit_dum.iloc[:, [x, -3, -2, -1]] for x in range(0, 4)]\n",
    "\n",
    "    Y = [df_wit_dum.iloc[:, [y]] for y in range(4, 11)]\n",
    "\n",
    "    slopes_dict = {}\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "\n",
    "        name_of_score = X_names[i][0].split()[0]\n",
    "        slopes_name = f'{name_of_score}_slopes'\n",
    "        \n",
    "        slopes = []\n",
    "        for y in Y:\n",
    "            slope = fit_regression(y, x)\n",
    "            slopes.append(slope)\n",
    "\n",
    "        slopes_dict[slopes_name] = pl.Series(company_name, slopes)\n",
    "    \n",
    "    return slopes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the general loop that takes dict_for_anal() whith scores and returns and for each company in the dict it calculates two series: first one contain regression slopes for weighted scores and second one for raw scores. Serieses stacked in the separete dataframes (for raw and for weighted scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_slopes(dict_for_anal):\n",
    "\n",
    "    slopes_df_dict = {\n",
    "        \"weighted_slopes_df\" : pl.DataFrame(),\n",
    "        \"raw_slopes_df\" : pl.DataFrame(),\n",
    "        \"lm_slopes_df\" : pl.DataFrame(),\n",
    "        \"harvard_slopes_df\" : pl.DataFrame()\n",
    "    }\n",
    "    \n",
    "    for company_name, company_df in dict_for_anal.items(): \n",
    "\n",
    "        X_names = [company_df.select(pl.col(\"row_names\")).row(x) for x in range(0, 4)]\n",
    "\n",
    "        company_df_woi = company_df.select(pl.exclude(\"row_names\"))\n",
    "\n",
    "        company_df_woi = company_df_woi.to_pandas()\n",
    "\n",
    "        df_wit_dum = add_quarter_dummy(company_df_woi)\n",
    "\n",
    "        slopes_dict = regression_slopes(df_wit_dum, X_names, company_name)\n",
    "\n",
    "        for x in X_names:\n",
    "            \n",
    "            name_of_score = x[0].split()[0]\n",
    "            slopes_name = f'{name_of_score}_slopes'\n",
    "            \n",
    "            df_name = f\"{name_of_score}_slopes_df\"\n",
    "            \n",
    "            slopes_df_dict[df_name] = slopes_df_dict[df_name].hstack([slopes_dict[slopes_name]])\n",
    "\n",
    "    return slopes_df_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_df_dict = compute_all_slopes(dict_for_anal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an output from previous functions we have dataframes with regression slopes of different returns on raw, weighted scores and Loughran-McDonald and Harvard dictionaries. Each col of these dfs contain regression slopes for particular company. First row contain slope for 2_days return, second row contain slope for 3_days return, etc.  I have decided to check what is the proportion of positive regression slopes for each return timeframe (2_days, 3_days, etc). For that purpose i will transpose each of these dfs, apply condition > 0, then verticaly sum boolean values and obtain the prorortion of positive slopes for each time frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_efficiency_metrics(slopes_df):\n",
    "\n",
    "    df_tranposed = slopes_df.transpose()\n",
    "\n",
    "    res = df_tranposed.select(pl.all() > 0).sum()\n",
    "    total_num_of_slopes = slopes_df.shape[1]\n",
    "\n",
    "    positive_slopes = [res.select(pl.col(res.columns[x]).gather(0)).item() for x in range(slopes_df.shape[0])]\n",
    "\n",
    "    for counter, x in enumerate(positive_slopes):\n",
    "        print(f'Prorortion of positive {counter + 2}_day returns: {x/total_num_of_slopes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop apply calc_efficiency_metrics function to each item of slopes_df_dict where each item is a dataframe which regression slopes for each regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Metrics for: weighted_slopes_df\n",
      "Prorortion of positive 2_day returns: 0.65\n",
      "Prorortion of positive 3_day returns: 0.55\n",
      "Prorortion of positive 4_day returns: 0.5\n",
      "Prorortion of positive 5_day returns: 0.5\n",
      "Prorortion of positive 6_day returns: 0.5\n",
      "Prorortion of positive 7_day returns: 0.55\n",
      "Prorortion of positive 8_day returns: 0.55\n",
      "\n",
      "\n",
      "Metrics for: raw_slopes_df\n",
      "Prorortion of positive 2_day returns: 0.65\n",
      "Prorortion of positive 3_day returns: 0.45\n",
      "Prorortion of positive 4_day returns: 0.5\n",
      "Prorortion of positive 5_day returns: 0.45\n",
      "Prorortion of positive 6_day returns: 0.5\n",
      "Prorortion of positive 7_day returns: 0.55\n",
      "Prorortion of positive 8_day returns: 0.55\n",
      "\n",
      "\n",
      "Metrics for: lm_slopes_df\n",
      "Prorortion of positive 2_day returns: 0.55\n",
      "Prorortion of positive 3_day returns: 0.45\n",
      "Prorortion of positive 4_day returns: 0.4\n",
      "Prorortion of positive 5_day returns: 0.5\n",
      "Prorortion of positive 6_day returns: 0.55\n",
      "Prorortion of positive 7_day returns: 0.55\n",
      "Prorortion of positive 8_day returns: 0.5\n",
      "\n",
      "\n",
      "Metrics for: harvard_slopes_df\n",
      "Prorortion of positive 2_day returns: 0.55\n",
      "Prorortion of positive 3_day returns: 0.45\n",
      "Prorortion of positive 4_day returns: 0.4\n",
      "Prorortion of positive 5_day returns: 0.5\n",
      "Prorortion of positive 6_day returns: 0.5\n",
      "Prorortion of positive 7_day returns: 0.55\n",
      "Prorortion of positive 8_day returns: 0.4\n"
     ]
    }
   ],
   "source": [
    "for key, slopes_df in slopes_df_dict.items():\n",
    "    print(\"\\n\")\n",
    "    print(f'Metrics for: {key}')\n",
    "    calc_efficiency_metrics(slopes_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive slopes means that sentiment scores and returns mooves in the same direction (hense, they are working as predictors of stock price movement). As the result of this preliminary analisis we can say that sentiment scores of quarterly reports are not very good predictors of stock movments. Positive proportion of this slopes are close to 0.5, which means that on average in our sample there are 50/50 chance that scores and returns are mooving in the same direction.\n",
    "\n",
    "As we can see that, despite visual conclusion that weighted scores are more \"smooth\", they barely outperform raw scores, but at least they are not worse, meaning that at each time frame (except 5_days) the proportion of positive regression slopes of weighted scores => proportion of positive regression slopes of raw scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This func stackes all data in one df, and calculate the single slope for each time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signl_slope(dict_for_anal):\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    for company_name in dict_for_anal:\n",
    "\n",
    "\n",
    "        X_names = [dict_for_anal[company_name].select(pl.col(\"row_names\")).row(x) for x in range(0, 4)] \n",
    "    \n",
    "        company_df_woi = dict_for_anal[company_name].select(pl.exclude(\"row_names\"))\n",
    "        company_df_pd = company_df_woi.to_pandas(use_pyarrow_extension_array=True)\n",
    "\n",
    "        df_wit_dum = add_quarter_dummy(company_df_pd)\n",
    "\n",
    "        df_list.append(df_wit_dum)\n",
    "        \n",
    "\n",
    "    total_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    slopes_dict = regression_slopes(total_df, X_names)\n",
    "\n",
    "    return slopes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weighted_slopes': shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.348815\n",
      "\t0.295345\n",
      "\t0.242682\n",
      "\t0.200228\n",
      "\t0.178836\n",
      "\t0.109004\n",
      "\t-0.012782\n",
      "], 'raw_slopes': shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.240884\n",
      "\t0.193844\n",
      "\t0.141836\n",
      "\t0.12746\n",
      "\t0.119962\n",
      "\t0.058712\n",
      "\t-0.020508\n",
      "], 'lm_slopes': shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.372185\n",
      "\t0.101023\n",
      "\t0.009957\n",
      "\t0.056129\n",
      "\t0.056805\n",
      "\t0.057414\n",
      "\t-0.003471\n",
      "], 'harvard_slopes': shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.489367\n",
      "\t0.002391\n",
      "\t-0.143786\n",
      "\t-0.047317\n",
      "\t-0.009509\n",
      "\t-0.006875\n",
      "\t-0.059141\n",
      "]}\n"
     ]
    }
   ],
   "source": [
    "slopes_dict = compute_signl_slope(dict_for_anal)\n",
    "print(slopes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop iterate over slopes_dict where now each item is not a datafame but a series of values, where each series contain regression slopes for the whole data (data for different companies stacked in one dataframe) for each time frame of returns: 2-days, 3-days... 7_days, full quarter returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_slopes for all data stacked displayed: \n",
      " shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.348815\n",
      "\t0.295345\n",
      "\t0.242682\n",
      "\t0.200228\n",
      "\t0.178836\n",
      "\t0.109004\n",
      "\t-0.012782\n",
      "]\n",
      "\n",
      "\n",
      "raw_slopes for all data stacked displayed: \n",
      " shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.240884\n",
      "\t0.193844\n",
      "\t0.141836\n",
      "\t0.12746\n",
      "\t0.119962\n",
      "\t0.058712\n",
      "\t-0.020508\n",
      "]\n",
      "\n",
      "\n",
      "lm_slopes for all data stacked displayed: \n",
      " shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.372185\n",
      "\t0.101023\n",
      "\t0.009957\n",
      "\t0.056129\n",
      "\t0.056805\n",
      "\t0.057414\n",
      "\t-0.003471\n",
      "]\n",
      "\n",
      "\n",
      "harvard_slopes for all data stacked displayed: \n",
      " shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.489367\n",
      "\t0.002391\n",
      "\t-0.143786\n",
      "\t-0.047317\n",
      "\t-0.009509\n",
      "\t-0.006875\n",
      "\t-0.059141\n",
      "]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for slope in slopes_dict:\n",
    "    print(f'{slope} for all data stacked displayed: \\n {slopes_dict[slope]}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter nbconvert --to pdf Parsim-sec\\src\\Analysis\\YFiin_excess_returns.ipynb\n",
    "\n",
    "jupyter nbconvert --to html Parsim-sec\\src\\Analysis\\YFiin_excess_returns.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
