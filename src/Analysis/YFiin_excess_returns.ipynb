{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import os\n",
    "import re\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME_LIST = ['Apple%2520Inc.%2520(AAPL)%2520(CIK%25200000320193)', 'MICROSOFT%2520CORP%2520(MSFT)%2520(CIK%25200000789019)', \n",
    "                     'BigCommerce%2520Holdings%252C%2520Inc.%2520(BIGC)%2520(CIK%25200001626450)', 'ROKU%252C%2520INC%2520(ROKU)%2520(CIK%25200001428439)', \n",
    "                     'JPMORGAN%2520CHASE%2520%2526%2520CO%2520(JPM%252C%2520AMJ%252C%2520AMJB%252C%2520JPM)%2520(CIK%25200000019617)', 'VISA%2520INC.%2520(V)%2520(CIK%25200001403161)', \n",
    "                     'Block%252C%2520Inc.%2520(SQ%252C%2520BSQKZ)%2520(CIK%25200001512673)', 'Robinhood%2520Markets%252C%2520Inc.%2520(HOOD)%2520(CIK%25200001783879)', \n",
    "                     'JOHNSON%252C%252C%2520JOHNSON%2520(JNJ)%2520(CIK%25200000200406)', 'PFIZER%2520INC%2520(PFE)%2520(CIK%25200000078003)', \n",
    "                     'Moderna%252C%2520Inc.%2520(MRNA)%2520(CIK%25200001682852)', 'Teladoc%2520Health%252C%2520Inc.%2520(TDOC)%2520(CIK%25200001477449)', \n",
    "                     'EXXON%2520MOBIL%2520CORP%2520(XOM)%2520(CIK%25200000034088)', 'CHEVRON%2520CORP%2520(CVX)%2520(CIK%25200000093410)', \n",
    "                     'FIRST%2520SOLAR%252C%2520INC.%2520(FSLR)%2520(CIK%25200001274494)', 'PLUG%2520POWER%2520INC%2520(PLUG)%2520(CIK%25200001093691)', \n",
    "                     'GENERAL%252CELECTRIC%2520CO%2520(GE)%2520(CIK%25200000040545)', '3M%2520CO%2520(MMM)%2520(CIK%25200000066740)', \n",
    "                     'CATERPILLAR%2520INC%2520(CAT)%2520(CIK%25200000018230)', 'FASTENAL%2520CO%2520(FAST)%2520(CIK%25200000815556)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect scores from hard memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_scores = {}\n",
    "regex = r\"\\\\([^\\\\]+)_full_doc_sen_score_df\\.parquet$\"\n",
    "\n",
    "for root, dirs, files in os.walk(r'C:\\Users\\310\\Desktop\\Progects_Py\\Parsim-sec\\src\\Analysis\\data\\full_10_Q_scores'):\n",
    "    for file in files:\n",
    "            \n",
    "            file_path = os.path.join(root, file)\n",
    "            ticker = re.search(regex, file_path).group(1)\n",
    "            \n",
    "            df = pl.read_parquet(file_path)\n",
    "\n",
    "            df_dict_scores[ticker] = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2019-04-30</th><th>2019-07-31</th><th>2019-10-30</th><th>2020-02-24</th><th>2020-04-29</th><th>2020-07-29</th><th>2020-10-28</th><th>2021-02-12</th><th>2021-04-27</th><th>2021-07-27</th><th>2021-10-26</th><th>2022-02-11</th><th>2022-04-26</th><th>2022-07-26</th><th>2022-10-25</th><th>2023-02-10</th><th>2023-04-25</th><th>2023-07-25</th><th>2023-10-24</th><th>2024-02-02</th><th>2024-04-23</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>-12.849008</td><td>-10.494805</td><td>-14.107974</td><td>-30.133511</td><td>-13.439952</td><td>-12.999193</td><td>-14.971735</td><td>-35.735787</td><td>-10.841224</td><td>-10.056447</td><td>-8.511603</td><td>-30.035251</td><td>-3.151193</td><td>-2.180779</td><td>-4.783131</td><td>-26.817862</td><td>-4.257227</td><td>-5.295587</td><td>-3.992991</td><td>-29.066429</td><td>-20.433151</td></tr><tr><td>-84.0</td><td>-68.0</td><td>-77.0</td><td>-168.0</td><td>-81.0</td><td>-73.0</td><td>-84.0</td><td>-195.0</td><td>-73.0</td><td>-65.0</td><td>-55.0</td><td>-169.0</td><td>-40.0</td><td>-28.0</td><td>-38.0</td><td>-160.0</td><td>-35.0</td><td>-43.0</td><td>-38.0</td><td>-161.0</td><td>-121.0</td></tr><tr><td>-0.367816</td><td>-0.335992</td><td>-0.348473</td><td>-0.29972</td><td>-0.366922</td><td>-0.363445</td><td>-0.350427</td><td>-0.325886</td><td>-0.354839</td><td>-0.307278</td><td>-0.264813</td><td>-0.282203</td><td>-0.256506</td><td>-0.23964</td><td>-0.295455</td><td>-0.298453</td><td>-0.230088</td><td>-0.289176</td><td>-0.244094</td><td>-0.27239</td><td>-0.361179</td></tr><tr><td>0.457193</td><td>0.434381</td><td>0.430875</td><td>0.445714</td><td>0.463025</td><td>0.404468</td><td>0.414253</td><td>0.446668</td><td>0.456579</td><td>0.441176</td><td>0.443396</td><td>0.439415</td><td>0.40296</td><td>0.387564</td><td>0.37332</td><td>0.429854</td><td>0.469191</td><td>0.416539</td><td>0.409936</td><td>0.437905</td><td>0.408483</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ 2019-04-3 ┆ 2019-07-3 ┆ 2019-10-3 ┆ 2020-02-2 ┆ … ┆ 2023-07-2 ┆ 2023-10-2 ┆ 2024-02-0 ┆ 2024-04- │\n",
       "│ 0         ┆ 1         ┆ 0         ┆ 4         ┆   ┆ 5         ┆ 4         ┆ 2         ┆ 23       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ -12.84900 ┆ -10.49480 ┆ -14.10797 ┆ -30.13351 ┆ … ┆ -5.295587 ┆ -3.992991 ┆ -29.06642 ┆ -20.4331 │\n",
       "│ 8         ┆ 5         ┆ 4         ┆ 1         ┆   ┆           ┆           ┆ 9         ┆ 51       │\n",
       "│ -84.0     ┆ -68.0     ┆ -77.0     ┆ -168.0    ┆ … ┆ -43.0     ┆ -38.0     ┆ -161.0    ┆ -121.0   │\n",
       "│ -0.367816 ┆ -0.335992 ┆ -0.348473 ┆ -0.29972  ┆ … ┆ -0.289176 ┆ -0.244094 ┆ -0.27239  ┆ -0.36117 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
       "│ 0.457193  ┆ 0.434381  ┆ 0.430875  ┆ 0.445714  ┆ … ┆ 0.416539  ┆ 0.409936  ┆ 0.437905  ┆ 0.408483 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict_scores[\"GE\"]\n",
    "tic_sym = \"GE\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is support func to find end prices and their dates. It takes as an arguments starting index, which is the nuber of row of hist dataframe and this hist dataframe itself. Hist dataframe contain daily OHLC data for particular ticker (company). \n",
    "\n",
    "It also cheks wether index of end price is in hist, meaning that if this particular report was released very resently (less then 8 trading days ago) it will append none values for end prices out of range of hist df instead of raising an error.\n",
    "\n",
    "It returns lists of end prices and corresponding dates (none values for prices and their dates, if they are out of range of hist df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_price(start_index, hist):\n",
    "    end_price_list = []\n",
    "    end_price_date_list = []\n",
    "    \n",
    "    for x in range(2, 8):\n",
    "        idx = start_index + x\n",
    "        if idx < len(hist):\n",
    "            end_price_list.append(hist.iloc[idx]['Open'])\n",
    "            end_price_date_list.append(hist.index[idx])\n",
    "        else:\n",
    "            end_price_list.append(None)\n",
    "            end_price_date_list.append(None)\n",
    "    return end_price_list, end_price_date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is support func to calc the regular returns. It takes start price and end_price_list as an arguments. It checks whether value in end price list is none and if it is, it appends none in returns list, instead of raising an error. So we are shure that all end price lists are of the same length, but for those prices that are not found in historical data we have none values for returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_returns(start_price, end_price_list):\n",
    "    retuns = []\n",
    "    for end_price in end_price_list:\n",
    "        if end_price is None:\n",
    "            retuns.append(None)\n",
    "        else:\n",
    "            ret = (end_price - start_price) / start_price * 100\n",
    "            retuns.append(ret)\n",
    "    return retuns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the support func that calc SnP500 returns. It takes_start date and end_price_date_list as and arguments and then calculate returns for this timeframes. This func also checks whether value of end_price_date_list is none (meaning that it is not found in historical data), and if it is, func will append none instead of raising an error.\n",
    "\n",
    "Now we are shure that list of snp_returns will be of equal length for all report release dates and all time frames, but for those returns that cannot be calculates due to non existent end prices we will have nones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snp_500_return(start_date, end_price_date_list):\n",
    "    snp500_ticker = yf.Ticker(\"^GSPC\")\n",
    "    snp_price = snp500_ticker.history(period=\"5y\", auto_adjust=True)\n",
    "\n",
    "    snp_returns = []\n",
    "\n",
    "    for end_date in end_price_date_list:\n",
    "        if end_date is None:\n",
    "            snp_returns.append(None)\n",
    "\n",
    "        else: \n",
    "            start_index = snp_price.index.get_loc(start_date)\n",
    "            start_price = snp_price.iloc[start_index]['Open']\n",
    "    \n",
    "            end_index = snp_price.index.get_loc(end_date)\n",
    "            end_price = snp_price.iloc[end_index]['Open']\n",
    "    \n",
    "            ret = (end_price - start_price) / start_price * 100\n",
    "            \n",
    "            snp_returns.append(ret)\n",
    "\n",
    "    return snp_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is suppurt function that retrive the end quarter price. It take as an argument hist - historical price dataframe, date_str - the date of release of this particular report, df - company dataframe, with cols named with dates of releases of corresponding reports.\n",
    "\n",
    "It takes the release date for current report and checks whether it is the last date in time series or not. If it is (this mean that we are are realy close to the end of hist df) we consider the end of hist df as the end of quarter. if it is not, we take next date in company df (which is the date of releas of the next quarterly report) and see its open price, this is pricisely the end of the quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_quarter(hist, date_str, df):\n",
    "\n",
    "    col_names = df.columns\n",
    "    current_index = col_names.index(date_str)\n",
    "\n",
    "    if current_index < len(col_names) - 1:\n",
    "\n",
    "        next_date = col_names[current_index + 1]\n",
    "\n",
    "        next_date_time_stemp = pd.Timestamp(next_date, tz='America/New_York')\n",
    "\n",
    "        while next_date_time_stemp not in hist.index:\n",
    "\n",
    "            next_date_time_stemp += pd.Timedelta(days=1)\n",
    "    \n",
    "        end_quarter_index = hist.index.get_loc(next_date_time_stemp)\n",
    "        end_quarter_price = hist.iloc[end_quarter_index]['Open']\n",
    "        end_quarter_date = hist.index[end_quarter_index]\n",
    "\n",
    "    else:\n",
    "        end_quarter_price = hist.iloc[-1]['Open']\n",
    "        end_quarter_date = hist.index[-1]\n",
    "\n",
    "    return end_quarter_price, end_quarter_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a func that takes as an input a key-value pair from df_dict_scores and calculate 2-days, 3-days, 4-days, 5-days, 6-days, 7-days and full quarter excess returns for it (S&P500 is the benchmark). It stores the result as a polars df with dates of starting of the period as col names and corresponding returns as col values (seven in each: starting from 2-days down to 7-days and full quarter) - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_returns(tic_sym, df):\n",
    "\n",
    "    company_ticker = yf.Ticker(tic_sym)\n",
    "    hist = company_ticker.history(period=\"5y\")\n",
    "    \n",
    "    returns = {}\n",
    "\n",
    "    for date_str in df.columns:\n",
    "\n",
    "        start_date = pd.Timestamp(date_str, tz='America/New_York')\n",
    "\n",
    "        while start_date not in hist.index:\n",
    "\n",
    "            start_date += pd.Timedelta(days=1)\n",
    "\n",
    "        start_index = hist.index.get_loc(start_date)\n",
    "\n",
    "        start_price = hist.iloc[start_index]['Open']\n",
    "\n",
    "        end_price_list, end_price_date_list = find_end_price(start_index, hist)\n",
    "\n",
    "        if None not in end_price_list:\n",
    "            end_quarter_price, end_quarter_date = find_end_quarter(hist, date_str, df)\n",
    "        else:\n",
    "            end_quarter_price = None\n",
    "            end_quarter_date = None\n",
    "        \n",
    "        # additional check if end of quarter is withing 8 days horison, and if it is, to avoid double counting we append none instead of last value of hist df\n",
    "        if end_quarter_price != end_price_list[-1]:\n",
    "\n",
    "            end_price_list.append(end_quarter_price)\n",
    "            end_price_date_list.append(end_quarter_date)\n",
    "        else:\n",
    "            end_price_list.append(None)\n",
    "            end_price_date_list.append(None)\n",
    "        \n",
    "        reg_returns = regular_returns(start_price, end_price_list)\n",
    "        snp_returns = snp_500_return(start_date, end_price_date_list)\n",
    "\n",
    "        excess_returns = [a - b if a is not None and b is not None else None for a, b in zip(reg_returns, snp_returns)]\n",
    "\n",
    "        returns[date_str] = excess_returns\n",
    "\n",
    "    returns = pl.DataFrame(returns)\n",
    "    \n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2019-04-30</th><th>2019-07-31</th><th>2019-10-30</th><th>2020-02-24</th><th>2020-04-29</th><th>2020-07-29</th><th>2020-10-28</th><th>2021-02-12</th><th>2021-04-27</th><th>2021-07-27</th><th>2021-10-26</th><th>2022-02-11</th><th>2022-04-26</th><th>2022-07-26</th><th>2022-10-25</th><th>2023-02-10</th><th>2023-04-25</th><th>2023-07-25</th><th>2023-10-24</th><th>2024-02-02</th><th>2024-04-23</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>-12.849008</td><td>-10.494805</td><td>-14.107974</td><td>-30.133511</td><td>-13.439952</td><td>-12.999193</td><td>-14.971735</td><td>-35.735787</td><td>-10.841224</td><td>-10.056447</td><td>-8.511603</td><td>-30.035251</td><td>-3.151193</td><td>-2.180779</td><td>-4.783131</td><td>-26.817862</td><td>-4.257227</td><td>-5.295587</td><td>-3.992991</td><td>-29.066429</td><td>-20.433151</td></tr><tr><td>-84.0</td><td>-68.0</td><td>-77.0</td><td>-168.0</td><td>-81.0</td><td>-73.0</td><td>-84.0</td><td>-195.0</td><td>-73.0</td><td>-65.0</td><td>-55.0</td><td>-169.0</td><td>-40.0</td><td>-28.0</td><td>-38.0</td><td>-160.0</td><td>-35.0</td><td>-43.0</td><td>-38.0</td><td>-161.0</td><td>-121.0</td></tr><tr><td>-0.367816</td><td>-0.335992</td><td>-0.348473</td><td>-0.29972</td><td>-0.366922</td><td>-0.363445</td><td>-0.350427</td><td>-0.325886</td><td>-0.354839</td><td>-0.307278</td><td>-0.264813</td><td>-0.282203</td><td>-0.256506</td><td>-0.23964</td><td>-0.295455</td><td>-0.298453</td><td>-0.230088</td><td>-0.289176</td><td>-0.244094</td><td>-0.27239</td><td>-0.361179</td></tr><tr><td>0.457193</td><td>0.434381</td><td>0.430875</td><td>0.445714</td><td>0.463025</td><td>0.404468</td><td>0.414253</td><td>0.446668</td><td>0.456579</td><td>0.441176</td><td>0.443396</td><td>0.439415</td><td>0.40296</td><td>0.387564</td><td>0.37332</td><td>0.429854</td><td>0.469191</td><td>0.416539</td><td>0.409936</td><td>0.437905</td><td>0.408483</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ 2019-04-3 ┆ 2019-07-3 ┆ 2019-10-3 ┆ 2020-02-2 ┆ … ┆ 2023-07-2 ┆ 2023-10-2 ┆ 2024-02-0 ┆ 2024-04- │\n",
       "│ 0         ┆ 1         ┆ 0         ┆ 4         ┆   ┆ 5         ┆ 4         ┆ 2         ┆ 23       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ -12.84900 ┆ -10.49480 ┆ -14.10797 ┆ -30.13351 ┆ … ┆ -5.295587 ┆ -3.992991 ┆ -29.06642 ┆ -20.4331 │\n",
       "│ 8         ┆ 5         ┆ 4         ┆ 1         ┆   ┆           ┆           ┆ 9         ┆ 51       │\n",
       "│ -84.0     ┆ -68.0     ┆ -77.0     ┆ -168.0    ┆ … ┆ -43.0     ┆ -38.0     ┆ -161.0    ┆ -121.0   │\n",
       "│ -0.367816 ┆ -0.335992 ┆ -0.348473 ┆ -0.29972  ┆ … ┆ -0.289176 ┆ -0.244094 ┆ -0.27239  ┆ -0.36117 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
       "│ 0.457193  ┆ 0.434381  ┆ 0.430875  ┆ 0.445714  ┆ … ┆ 0.416539  ┆ 0.409936  ┆ 0.437905  ┆ 0.408483 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict_scores[\"GE\"]\n",
    "tic_sym = \"GE\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>2019-04-30</th><th>2019-07-31</th><th>2019-10-30</th><th>2020-02-24</th><th>2020-04-29</th><th>2020-07-29</th><th>2020-10-28</th><th>2021-02-12</th><th>2021-04-27</th><th>2021-07-27</th><th>2021-10-26</th><th>2022-02-11</th><th>2022-04-26</th><th>2022-07-26</th><th>2022-10-25</th><th>2023-02-10</th><th>2023-04-25</th><th>2023-07-25</th><th>2023-10-24</th><th>2024-02-02</th><th>2024-04-23</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.340799</td><td>-4.572537</td><td>2.706268</td><td>0.876016</td><td>1.541922</td><td>-11.92608</td><td>-0.800972</td><td>3.593513</td><td>-2.797514</td><td>-0.760631</td><td>-1.894776</td><td>0.66832</td><td>-6.423492</td><td>5.144775</td><td>0.805559</td><td>0.213406</td><td>-3.596391</td><td>-0.019744</td><td>2.060124</td><td>0.967731</td><td>0.343106</td></tr><tr><td>2.184347</td><td>-5.09771</td><td>5.25371</td><td>-3.164989</td><td>-2.14395</td><td>-14.05162</td><td>1.251449</td><td>3.660993</td><td>-2.75369</td><td>-0.79391</td><td>-0.638811</td><td>2.201697</td><td>-8.211534</td><td>1.372217</td><td>2.021194</td><td>0.984363</td><td>-4.805354</td><td>0.667746</td><td>0.326283</td><td>1.196205</td><td>0.905228</td></tr><tr><td>0.766435</td><td>-4.618149</td><td>10.305676</td><td>-4.425613</td><td>-4.289187</td><td>-14.675309</td><td>2.050589</td><td>2.473791</td><td>-2.014977</td><td>-1.809922</td><td>-0.697364</td><td>2.457382</td><td>-9.335515</td><td>1.169088</td><td>1.537658</td><td>2.338554</td><td>-3.391047</td><td>-0.70286</td><td>-0.90291</td><td>0.768599</td><td>1.579864</td></tr><tr><td>1.738562</td><td>-7.043549</td><td>10.595441</td><td>4.591411</td><td>-5.975396</td><td>-14.524466</td><td>0.482673</td><td>6.451623</td><td>-0.965278</td><td>-5.503712</td><td>-0.208393</td><td>2.094734</td><td>-6.768401</td><td>4.159919</td><td>2.052851</td><td>3.231999</td><td>-2.227282</td><td>-1.226391</td><td>0.135034</td><td>0.85115</td><td>2.140489</td></tr><tr><td>0.740317</td><td>-7.63825</td><td>11.941535</td><td>1.18299</td><td>-7.903998</td><td>-12.41519</td><td>1.572814</td><td>11.824766</td><td>-2.25133</td><td>-4.519519</td><td>-0.75663</td><td>-2.592296</td><td>-5.691225</td><td>3.605803</td><td>2.712954</td><td>2.053079</td><td>0.03264</td><td>-1.242731</td><td>-1.661404</td><td>0.781553</td><td>2.999662</td></tr><tr><td>-0.150567</td><td>-9.147206</td><td>13.460592</td><td>1.114329</td><td>-6.705964</td><td>-13.510495</td><td>1.828762</td><td>13.152224</td><td>-1.617785</td><td>-3.243056</td><td>-1.735835</td><td>-1.257606</td><td>-6.376591</td><td>1.802583</td><td>4.235444</td><td>3.319419</td><td>0.612184</td><td>-0.9945</td><td>-2.720633</td><td>2.193602</td><td>null</td></tr><tr><td>2.918849</td><td>-9.764418</td><td>12.498068</td><td>-32.37501</td><td>-5.742201</td><td>4.090323</td><td>35.077557</td><td>11.355421</td><td>-6.612007</td><td>-4.403702</td><td>-4.688516</td><td>-9.05017</td><td>-10.257628</td><td>11.692293</td><td>31.097998</td><td>24.893303</td><td>2.023443</td><td>4.086154</td><td>5.989993</td><td>43.989958</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ 2019-04-3 ┆ 2019-07-3 ┆ 2019-10-3 ┆ 2020-02-2 ┆ … ┆ 2023-07-2 ┆ 2023-10-2 ┆ 2024-02-0 ┆ 2024-04- │\n",
       "│ 0         ┆ 1         ┆ 0         ┆ 4         ┆   ┆ 5         ┆ 4         ┆ 2         ┆ 23       │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 1.340799  ┆ -4.572537 ┆ 2.706268  ┆ 0.876016  ┆ … ┆ -0.019744 ┆ 2.060124  ┆ 0.967731  ┆ 0.343106 │\n",
       "│ 2.184347  ┆ -5.09771  ┆ 5.25371   ┆ -3.164989 ┆ … ┆ 0.667746  ┆ 0.326283  ┆ 1.196205  ┆ 0.905228 │\n",
       "│ 0.766435  ┆ -4.618149 ┆ 10.305676 ┆ -4.425613 ┆ … ┆ -0.70286  ┆ -0.90291  ┆ 0.768599  ┆ 1.579864 │\n",
       "│ 1.738562  ┆ -7.043549 ┆ 10.595441 ┆ 4.591411  ┆ … ┆ -1.226391 ┆ 0.135034  ┆ 0.85115   ┆ 2.140489 │\n",
       "│ 0.740317  ┆ -7.63825  ┆ 11.941535 ┆ 1.18299   ┆ … ┆ -1.242731 ┆ -1.661404 ┆ 0.781553  ┆ 2.999662 │\n",
       "│ -0.150567 ┆ -9.147206 ┆ 13.460592 ┆ 1.114329  ┆ … ┆ -0.9945   ┆ -2.720633 ┆ 2.193602  ┆ null     │\n",
       "│ 2.918849  ┆ -9.764418 ┆ 12.498068 ┆ -32.37501 ┆ … ┆ 4.086154  ┆ 5.989993  ┆ 43.989958 ┆ null     │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computing_returns(tic_sym, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the general loop, that takes each key-value pair from df_dict_scores, applies computing_returns() to it, then verticaly stackes the result with orig df and stores it in the new dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "BIGC\n",
      "CAT\n",
      "CVX\n",
      "FAST\n",
      "FSLR\n",
      "GE\n",
      "HOOD\n",
      "JNJ\n",
      "JPM\n",
      "MMM\n",
      "MRNA\n",
      "MSFT\n",
      "PFE\n",
      "PLUG\n",
      "ROKU\n",
      "SQ\n",
      "TDOC\n",
      "V\n",
      "XOM\n"
     ]
    }
   ],
   "source": [
    "dict_for_anal = {}\n",
    "row_names = pl.Series(\"row_names\", [\"weighted scores\", \"raw scores\", \"lm scores\", \"harvard scores\", \"2_day_reterns\",\"3_day_reterns\", \"4_day_reterns\", \"5_day_reterns\", \"6_day_reterns\", \"7_day_reterns\", \"full_quarter_returns\"])\n",
    "\n",
    "for tic_sym, df in df_dict_scores.items():\n",
    "    print(tic_sym)\n",
    "\n",
    "    returns_df = computing_returns(tic_sym, df)\n",
    "    \n",
    "    stacked_df = df.vstack(returns_df)\n",
    "    stacked_df_with_indx = stacked_df.hstack([row_names])\n",
    "    \n",
    "    dict_for_anal[tic_sym] = stacked_df_with_indx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start the analisys. The general question is: how well sentiment scores and weighted scores can explain returns? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see them on the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company_name, scores in dict_for_anal.items():\n",
    "   \n",
    "    df_for_plt = scores.select(pl.exclude(\"row_names\"))\n",
    "\n",
    "    dates = [datetime.strptime(date, \"%Y-%m-%d\") for date in df_for_plt.columns]\n",
    "   \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    row_indices_to_plot = [0, 1, 4, -1]\n",
    "\n",
    "    for idx in row_indices_to_plot:\n",
    "        row = df_for_plt.row(idx, named=True)  \n",
    "        plt.plot(dates, list(row.values()), label=scores.row(idx)[-1])\n",
    "\n",
    "    plt.title(company_name)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance some time series exhibits at least piecewise dependence. \n",
    "\n",
    "Next step is to regress each return on weighted, raw scores and polarity scores from Loughran-McDonald and Harvard dictionaries.\n",
    "\n",
    "And also we can see that in cases of 10-K reports which are typicly much longer we can see strong pattern in raw scores which is smoothed in weighted scores. But still this pattern could be recognized. This can be interpreted as 10-K can contain more information about future prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This func takes as input key-value pair (company name and dataframe with sentiment scores and returns), regress each retern (2_days, 3_days, etc) on weightes sentiment score ans raw sentiment score. Then it collects the slopes (beta-coefficient) of returns regressed on raw score and on weighted score in the separate polars series (in the loop they will be staked in dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_regresssor_variable(x, company_df_woi):\n",
    "#    regressor = pl.Series(company_df_woi.row(x))\n",
    "#    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_slopes(company_df_woi, X_names, company_name=\"total_df\"):\n",
    "    \n",
    "    # x_1 - weighted score\n",
    "    # x_2 - raw score\n",
    "    # x_3 - lm score\n",
    "    # x_4 - harvard score\n",
    "    \n",
    "    #X_value_with_const = [sm.add_constant(x) for x in X_values]\n",
    "    x_1 = company_df_woi.row(0)\n",
    "    x_2 = company_df_woi.row(1)\n",
    "    x_3 = company_df_woi.row(2)\n",
    "    x_4 = company_df_woi.row(3)\n",
    "\n",
    "\n",
    "    x_1 = sm.add_constant(x_1)\n",
    "    x_2 = sm.add_constant(x_2)\n",
    "    x_3 = sm.add_constant(x_3)\n",
    "    x_4 = sm.add_constant(x_4)\n",
    "    \n",
    "    X = [x_1, x_2, x_3, x_4]\n",
    "    \n",
    "    Y = [company_df_woi.row(y) for y in range(4, 11)]\n",
    "\n",
    "    slopes_dict = {}\n",
    "\n",
    "    for i, x in enumerate(X):\n",
    "\n",
    "        name_of_score = X_names[i][0].split()[0]\n",
    "        slopes_name = f'{name_of_score}_slopes'\n",
    "        \n",
    "        slopes = []\n",
    "        for y in Y:\n",
    "            slope = fit_regression(y, x)\n",
    "            slopes.append(slope)\n",
    "\n",
    "        slopes_dict[slopes_name] = pl.Series(company_name, slopes)\n",
    "    \n",
    "    #print(slopes_dict)\n",
    "    \n",
    "    #slopes_raw = []\n",
    "    #for y in Y:\n",
    "    #    \n",
    "    #    model = sm.OLS(y, x_2, missing='drop').fit()\n",
    "    #    slope = model.params[1]\n",
    "    #    slopes_raw.append(slope)\n",
    "    #\n",
    "    #slopes_lm = []\n",
    "    #for y in Y:\n",
    "    #    \n",
    "    #    model = sm.OLS(y, x_3, missing='drop').fit()\n",
    "    #    slope = model.params[1]\n",
    "    #    slopes_lm.append(slope)\n",
    "#\n",
    "    #slopes_harvard = []\n",
    "    #for y in Y:\n",
    "    #    slope = fit_regression(y, x_4)\n",
    "    #    slopes_harvard.append(slope)\n",
    "#\n",
    "#\n",
    "    #slopes_weighted = pl.Series(company_name, slopes_weighted)\n",
    "    #slopes_raw = pl.Series(company_name, slopes_raw)\n",
    "    #slopes_lm = pl.Series(company_name, slopes_lm)\n",
    "    #slopes_harvard = pl.Series(company_name, slopes_harvard)\n",
    "    \n",
    "\n",
    "    return slopes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression(y, x):\n",
    "    #slope_name = f'{x_name}_slopes'\n",
    "    #slopes = []\n",
    "\n",
    "    model = sm.OLS(y, x, missing='drop').fit()\n",
    "    slope = model.params[1]\n",
    "    \n",
    "    return slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the general loop that takes dict_for_anal() whith scores and returns and for each company in the dict it calculates two series: first one contain regression slopes for weighted scores and second one for raw scores. Serieses stacked in the separete dataframes (for raw and for weighted scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_slopes(dict_for_anal):\n",
    "\n",
    "    slopes_df_dict = {\n",
    "        \"weighted_slopes_df\" : pl.DataFrame(),\n",
    "        \"raw_slopes_df\" : pl.DataFrame(),\n",
    "        \"lm_slopes_df\" : pl.DataFrame(),\n",
    "        \"harvard_slopes_df\" : pl.DataFrame()\n",
    "    }\n",
    "    \n",
    "    for company_name, company_df in dict_for_anal.items(): \n",
    "\n",
    "        X_names = [company_df.select(pl.col(\"row_names\")).row(x) for x in range(0, 4)]\n",
    "\n",
    "        company_df_woi = company_df.select(pl.exclude(\"row_names\"))\n",
    "\n",
    "        slopes_dict = regression_slopes(company_df_woi, X_names, company_name)\n",
    "\n",
    "        \n",
    "\n",
    "        for x in X_names:\n",
    "            \n",
    "            name_of_score = x[0].split()[0]\n",
    "            slopes_name = f'{name_of_score}_slopes'\n",
    "            \n",
    "            df_name = f\"{name_of_score}_slopes_df\"\n",
    "            #print(df_name)\n",
    "            #if slopes_dict.get(slopes_name) is not None:\n",
    "            slopes_df_dict[df_name] = slopes_df_dict[df_name].hstack([slopes_dict[slopes_name]])\n",
    "\n",
    "            #print(slopes_dict[slopes_name])\n",
    "#\n",
    "            #df_to_stack = slopes_df_dict[df_name]\n",
    "            #df_to_stack.hstack([slopes_dict[slopes_name]])\n",
    "            #print(df_to_stack)\n",
    "            \n",
    "            \n",
    "            #weighted_slopes_df = weighted_slopes_df.hstack([slopes_dict[name_of_score]])\n",
    "            #raw_slopes_df = raw_slopes_df.hstack([slopes_dict])\n",
    "            #lm_slopes_df = lm_slopes_df.hstack([slopes_dict])\n",
    "            #harvard_slopes_df = harvard_slopes_df.hstack([slopes_dict])\n",
    "\n",
    "    return slopes_df_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_df_dict = compute_all_slopes(dict_for_anal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weighted_slopes_df': shape: (7, 20)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬──────────┬───────────┬───────────┐\n",
       " │ AAPL      ┆ BIGC      ┆ CAT       ┆ CVX       ┆ … ┆ SQ        ┆ TDOC     ┆ V         ┆ XOM       │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---      ┆ ---       ┆ ---       │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64      ┆ f64       ┆ f64       │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪══════════╪═══════════╪═══════════╡\n",
       " │ 0.002215  ┆ -0.21629  ┆ 0.038467  ┆ 0.062339  ┆ … ┆ -0.835702 ┆ 0.010367 ┆ 0.009526  ┆ -0.011088 │\n",
       " │ 0.006125  ┆ -0.208663 ┆ 0.01983   ┆ 0.026582  ┆ … ┆ -0.596019 ┆ 0.056356 ┆ -0.003506 ┆ -0.11075  │\n",
       " │ 0.023419  ┆ -0.100102 ┆ -0.059649 ┆ -0.034409 ┆ … ┆ -0.889246 ┆ 0.107207 ┆ 0.013914  ┆ -0.100079 │\n",
       " │ 0.011895  ┆ -0.115847 ┆ -0.043526 ┆ -0.064624 ┆ … ┆ -0.897121 ┆ 0.07863  ┆ 0.009448  ┆ -0.167727 │\n",
       " │ 0.052131  ┆ -0.128337 ┆ -0.029508 ┆ -0.073136 ┆ … ┆ -0.998268 ┆ 0.098513 ┆ -0.027718 ┆ -0.12614  │\n",
       " │ 0.093022  ┆ -0.031402 ┆ -0.033474 ┆ -0.165765 ┆ … ┆ -0.937751 ┆ 0.023814 ┆ -0.012299 ┆ -0.25658  │\n",
       " │ -0.250228 ┆ 0.377492  ┆ 0.040824  ┆ 0.429251  ┆ … ┆ 1.723141  ┆ 0.16089  ┆ -0.115959 ┆ -0.176432 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴──────────┴───────────┴───────────┘,\n",
       " 'raw_slopes_df': shape: (7, 20)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬──────────┬───────────┬───────────┐\n",
       " │ AAPL      ┆ BIGC      ┆ CAT       ┆ CVX       ┆ … ┆ SQ        ┆ TDOC     ┆ V         ┆ XOM       │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---      ┆ ---       ┆ ---       │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64      ┆ f64       ┆ f64       │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪══════════╪═══════════╪═══════════╡\n",
       " │ -0.000045 ┆ -0.041137 ┆ 0.008511  ┆ 0.008586  ┆ … ┆ -0.260527 ┆ 0.003589 ┆ 0.002171  ┆ -0.003299 │\n",
       " │ 0.000433  ┆ -0.03952  ┆ 0.004101  ┆ -0.00301  ┆ … ┆ -0.187829 ┆ 0.015136 ┆ 0.000138  ┆ -0.020486 │\n",
       " │ 0.003586  ┆ -0.017563 ┆ -0.013622 ┆ -0.014852 ┆ … ┆ -0.274744 ┆ 0.027634 ┆ 0.00349   ┆ -0.020773 │\n",
       " │ 0.001758  ┆ -0.020438 ┆ -0.010722 ┆ -0.015471 ┆ … ┆ -0.287543 ┆ 0.020769 ┆ 0.003415  ┆ -0.031593 │\n",
       " │ 0.008822  ┆ -0.023116 ┆ -0.00892  ┆ -0.010913 ┆ … ┆ -0.316156 ┆ 0.025655 ┆ -0.003813 ┆ -0.02237  │\n",
       " │ 0.016033  ┆ -0.00358  ┆ -0.010188 ┆ -0.036868 ┆ … ┆ -0.305472 ┆ 0.007537 ┆ -0.000126 ┆ -0.042588 │\n",
       " │ -0.040138 ┆ 0.077281  ┆ 0.012149  ┆ 0.126343  ┆ … ┆ 0.446382  ┆ 0.043757 ┆ -0.021704 ┆ -0.024156 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴──────────┴───────────┴───────────┘,\n",
       " 'lm_slopes_df': shape: (7, 20)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ AAPL      ┆ BIGC      ┆ CAT       ┆ CVX       ┆ … ┆ SQ        ┆ TDOC      ┆ V         ┆ XOM      │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ -2.509059 ┆ -69.23237 ┆ 7.144098  ┆ -6.508938 ┆ … ┆ 137.691   ┆ -3.614617 ┆ -7.139277 ┆ 3.811781 │\n",
       " │           ┆ 7         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       " │ -2.245793 ┆ -97.77840 ┆ 4.729632  ┆ -5.72582  ┆ … ┆ 153.60978 ┆ -14.66298 ┆ -5.472025 ┆ -0.26787 │\n",
       " │           ┆ 8         ┆           ┆           ┆   ┆ 4         ┆ 9         ┆           ┆ 2        │\n",
       " │ 0.473217  ┆ -28.22811 ┆ -6.23079  ┆ -5.409784 ┆ … ┆ 134.26315 ┆ -14.12584 ┆ -2.470753 ┆ -5.68968 │\n",
       " │           ┆ 9         ┆           ┆           ┆   ┆ 9         ┆ 3         ┆           ┆ 6        │\n",
       " │ -0.45776  ┆ 29.659909 ┆ -3.343263 ┆ -3.067648 ┆ … ┆ 141.54892 ┆ -7.359    ┆ -0.975237 ┆ 0.560301 │\n",
       " │           ┆           ┆           ┆           ┆   ┆ 9         ┆           ┆           ┆          │\n",
       " │ 4.111624  ┆ 55.587882 ┆ 8.086238  ┆ 4.001513  ┆ … ┆ 142.01367 ┆ -26.49128 ┆ -2.734107 ┆ 5.301499 │\n",
       " │           ┆           ┆           ┆           ┆   ┆ 9         ┆ 7         ┆           ┆          │\n",
       " │ 12.122613 ┆ 43.19688  ┆ 4.486679  ┆ 8.068512  ┆ … ┆ 93.006956 ┆ -21.11980 ┆ -7.841548 ┆ 12.00722 │\n",
       " │           ┆           ┆           ┆           ┆   ┆           ┆ 4         ┆           ┆ 6        │\n",
       " │ -39.08288 ┆ -104.6162 ┆ 0.262996  ┆ 65.786935 ┆ … ┆ -234.0569 ┆ -43.21868 ┆ 14.538387 ┆ 3.006503 │\n",
       " │ 8         ┆ 33        ┆           ┆           ┆   ┆ 22        ┆           ┆           ┆          │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " 'harvard_slopes_df': shape: (7, 20)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ AAPL      ┆ BIGC      ┆ CAT       ┆ CVX       ┆ … ┆ SQ        ┆ TDOC      ┆ V         ┆ XOM      │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 9.744509  ┆ -36.57061 ┆ -5.289298 ┆ -15.2709  ┆ … ┆ 192.83156 ┆ -5.793086 ┆ 7.740002  ┆ -0.69788 │\n",
       " │           ┆ 3         ┆           ┆           ┆   ┆ 3         ┆           ┆           ┆          │\n",
       " │ 11.26934  ┆ -44.30770 ┆ -41.71278 ┆ -13.41186 ┆ … ┆ 188.83021 ┆ 20.236315 ┆ 14.416263 ┆ -2.72051 │\n",
       " │           ┆ 6         ┆ 9         ┆ 5         ┆   ┆ 3         ┆           ┆           ┆ 6        │\n",
       " │ 13.209042 ┆ 7.285468  ┆ -86.68922 ┆ -25.65292 ┆ … ┆ 192.42340 ┆ 42.540024 ┆ 19.915595 ┆ -7.49026 │\n",
       " │           ┆           ┆ 1         ┆ 3         ┆   ┆ 2         ┆           ┆           ┆ 6        │\n",
       " │ 11.998138 ┆ 18.547874 ┆ -79.22870 ┆ -15.96867 ┆ … ┆ 199.55573 ┆ 32.676196 ┆ 19.398709 ┆ 3.137353 │\n",
       " │           ┆           ┆ 5         ┆           ┆   ┆ 3         ┆           ┆           ┆          │\n",
       " │ 18.254155 ┆ 16.159676 ┆ -67.02348 ┆ -3.849418 ┆ … ┆ 209.10456 ┆ 31.830617 ┆ 8.038924  ┆ 3.068323 │\n",
       " │           ┆           ┆ 8         ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       " │ 26.189543 ┆ 51.201496 ┆ -65.74384 ┆ 1.704274  ┆ … ┆ 168.96467 ┆ -4.284045 ┆ 10.484952 ┆ 14.11664 │\n",
       " │           ┆           ┆ 3         ┆           ┆   ┆ 2         ┆           ┆           ┆ 7        │\n",
       " │ -63.66631 ┆ 148.00896 ┆ -103.5409 ┆ 28.350079 ┆ … ┆ -328.1173 ┆ 92.250276 ┆ 21.14439  ┆ -68.6644 │\n",
       " │ 7         ┆ 9         ┆ 5         ┆           ┆   ┆ 83        ┆           ┆           ┆ 73       │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slopes_df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an output from previous functions we have two dataframes with regression slopes of different returns on raw and on weighted scores. Each col of these dfs contain regression slopes for particular company. First row contain slope for 2_days return, second row contain slope for 3_days return, etc.  I have decided to check what is the proportion of positive regression slopes for each return timeframe (2_days, 3_days, etc). For that purpose i will transpose each of these dfs, apply condition > 0, then verticaly sum boolean values and obtain the prorortion of positive slopes for each time frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_efficiency_metrics(slopes_df):\n",
    "\n",
    "    df_tranposed = slopes_df.transpose()\n",
    "\n",
    "    res = df_tranposed.select(pl.all() > 0).sum()\n",
    "    total_num_of_slopes = slopes_df.shape[1]\n",
    "\n",
    "    positive_slopes = [res.select(pl.col(res.columns[x]).gather(0)).item() for x in range(slopes_df.shape[0])]\n",
    "\n",
    "    for counter, x in enumerate(positive_slopes):\n",
    "        print(f'Prorortion of positive {counter + 2}_day returns: {x/total_num_of_slopes}')\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prorortion of positive 2_day returns: 0.5\n",
      "Prorortion of positive 3_day returns: 0.5\n",
      "Prorortion of positive 4_day returns: 0.45\n",
      "Prorortion of positive 5_day returns: 0.45\n",
      "Prorortion of positive 6_day returns: 0.4\n",
      "Prorortion of positive 7_day returns: 0.4\n",
      "Prorortion of positive 8_day returns: 0.5\n"
     ]
    }
   ],
   "source": [
    "calc_efficiency_metrics(raw_slopes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prorortion of positive 2_day returns: 0.55\n",
      "Prorortion of positive 3_day returns: 0.5\n",
      "Prorortion of positive 4_day returns: 0.45\n",
      "Prorortion of positive 5_day returns: 0.45\n",
      "Prorortion of positive 6_day returns: 0.4\n",
      "Prorortion of positive 7_day returns: 0.4\n",
      "Prorortion of positive 8_day returns: 0.5\n"
     ]
    }
   ],
   "source": [
    "calc_efficiency_metrics(weighted_slopes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prorortion of positive 2_day returns: 0.45\n",
      "Prorortion of positive 3_day returns: 0.5\n",
      "Prorortion of positive 4_day returns: 0.6\n",
      "Prorortion of positive 5_day returns: 0.65\n",
      "Prorortion of positive 6_day returns: 0.6\n",
      "Prorortion of positive 7_day returns: 0.65\n",
      "Prorortion of positive 8_day returns: 0.5\n"
     ]
    }
   ],
   "source": [
    "calc_efficiency_metrics(harvard_slopes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prorortion of positive 2_day returns: 0.45\n",
      "Prorortion of positive 3_day returns: 0.5\n",
      "Prorortion of positive 4_day returns: 0.6\n",
      "Prorortion of positive 5_day returns: 0.65\n",
      "Prorortion of positive 6_day returns: 0.6\n",
      "Prorortion of positive 7_day returns: 0.65\n",
      "Prorortion of positive 8_day returns: 0.5\n"
     ]
    }
   ],
   "source": [
    "calc_efficiency_metrics(harvard_slopes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive slopes means that sentiment scores and returns mooves in the same direction (hense, they are working as predictors of stock price movement). As the result of this preliminary analisis we can say that sentiment scores of 10-Q reports are not very good predictors of stock movments. Positive proportion of this slopes are close to 0.5, which means that on average in our sample there are 50/50 chance that scores and returns are mooving in the same direction.\n",
    "\n",
    "As we can see that, despite visual conclusion that weighted scores are more \"smooth\", they barely outperform raw scores, but at least they are not worse, meaning that at each time frame (except 5_days) the proportion of positive regression slopes of weighted scores => proportion of positive regression slopes of raw scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This func stackes all data in one df, and calculate the single slope for each time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signl_slope(dict_for_anal):\n",
    "\n",
    "    total_df = pl.DataFrame()\n",
    "    \n",
    "    for company_name, company_df in dict_for_anal.items(): \n",
    "    \n",
    "        company_df = company_df.select(pl.exclude(\"row_names\"))\n",
    "        \n",
    "        company_df = company_df.rename({col: f\"{col}_{company_name}\" for col in company_df.columns})\n",
    "\n",
    "        total_df = total_df.hstack(company_df)\n",
    "    \n",
    "    slope_weighted, slope_raw, slope_lm, slope_harvard = regression_slopes(total_df)\n",
    "\n",
    "    return slope_weighted, slope_raw, slope_lm, slope_harvard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_weighted, slopes_raw, slopes_lm, slopes_harvard = compute_signl_slope(dict_for_anal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " slopes_weighted: shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t-0.009537\n",
      "\t0.017868\n",
      "\t0.027616\n",
      "\t0.021722\n",
      "\t0.021401\n",
      "\t0.010745\n",
      "\t0.06722\n",
      "], slopes_raw: shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t-0.00411\n",
      "\t0.002284\n",
      "\t0.004373\n",
      "\t0.002886\n",
      "\t0.002626\n",
      "\t0.000149\n",
      "\t0.015163\n",
      "], slopes_lm: shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t0.330616\n",
      "\t1.179053\n",
      "\t1.02975\n",
      "\t1.786038\n",
      "\t1.610909\n",
      "\t1.634859\n",
      "\t18.028693\n",
      "], slopes_harvard: shape: (7,)\n",
      "Series: 'total_df' [f64]\n",
      "[\n",
      "\t-3.739671\n",
      "\t0.576719\n",
      "\t4.261854\n",
      "\t3.209666\n",
      "\t4.483385\n",
      "\t3.945624\n",
      "\t16.088036\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(f' slopes_weighted: {slopes_weighted}, slopes_raw: {slopes_raw}, slopes_lm: {slopes_lm}, slopes_harvard: {slopes_harvard}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter nbconvert --to pdf Parsim-sec\\src\\Analysis\\YFiin_excess_returns.ipynb\n",
    "\n",
    "jupyter nbconvert --to html Parsim-sec\\src\\Analysis\\YFiin_excess_returns.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
